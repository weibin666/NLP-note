---
typora-root-url: images
---

# 自然语言处理-基于预训练模型的方法

公式使用方法：https://zhuanlan.zhihu.com/p/95886235

本书的目的：建立NLP知识框架，入门NLP

## 第一章 绪论

### 1.1 自然语言处理任务体系

按照从底层到高层的方式：

​						资源建设、基础任务、应用任务和应用系统。



![](/1-1.jpg)

附：

预训练模型(Pre-train)：即首先在一个原任务上预先训练一个初始模型，然后再下游任务(也称目标任务)上继续对该模型进行精调(Fine-tune)，从而达到提高下游任务准确率的目的。

### 1.2 任务类别

#### 1.2.1 回归问题

即将输入文本映射为一个连续的数值，如对作文的打分，对案件刑期或返款金额的预测等

#### 1.2.2 分类问题

又称文本分类，即判断一个输入的文本所属的类别，如：在垃圾邮件识别任务重，可以将一封邮件分为正常和垃圾两类；在情感分析中，可以将用户的情感分为褒义、贬义或中性三类。

#### 1.2.3 匹配问题

判断两个输入文本之间的关系，如：它们之间是复述或非复述两类关系；或者蕴含、矛盾和无关三类关系。

另外，是被两个输入文本之间的相似性(0到1的数值)也属于匹配问题。

#### 1.2.4 解析问题

特指对文本中的词语进行标注或识别词语之间的关系，典型的解析问题包括词性标注、句法分析等，另外还有很多问题，如分词、命名实体识别等也可以转化为解析问题。

#### 1.2.5 生成问题

特指根据输入(可以是文本，也可以是图片、表格等其他类型数据)生成一段自然语言，如机器翻译、文本摘要、图像描述生成等都是典型的文本生成类任务。

## 第二章 自然语言处理基础

词的表示经过的三个阶段：独热(one-hot)---->分布式----->词向量----->词袋模型

### 2.1 文本的表示

自然语言处理中，我们将文本表示为向量，其中的每一维代表一个特征。

以情感极性识别为例，其表示过程：

​	令向量X的每一维表示某个词在该文本中出现的次数，如x1表示“我”出现的次数，x2表示“喜欢”出现的次数，x3表示“电影”出现的次数，x4表示“讨厌”出现的次数等，如果某个词在该句中没有出现，则相应的维数被设置为0。可见，输入向量X的大小恰好为整个词表(所有不相同的词)的大小。然后就可以根据每个词对判断情感极性的重要性进行加权，如“喜欢”(x2)对应的权重w2可能比较大，而“讨厌”(x4)对应的权重w4可能比较小(可以为负数)，对于情感极性影响比较小的词，如“我”“电影”等，对应的权重可能会趋近于0。

这种文本表示的方法是两种技术的组合，即词的独热表示和文本的词袋表示。

#### 2.1.1 词的独热表示

词的独热表示：就是使用一个词表大小的向量表示一个词(假设词表为`V`，则大小为`|V|`)，然后将词表中的第i个词$w_i$表示为向量：

![](2-1.jpg)

独热表示的缺点：

1. 一个主要问题就是不同词使用完全不同的向量进行表示，这会导致即使两个词在语义上很相似，但是通过余弦函数来度量它们之间的相似度时值却为0;
2. 导致数据<font color="red">稀疏性</font>问题。

#### 2.1.2 词的分布式表示

词的含义可由其上下文的分布进行表示。是一种传统的机器学习方法。

##### 2.1.2.1 分布式假设

分布式表示缺点：

1. 高频词误导计算结果；
2. 共现频次无法反映词之间的高阶关系；
3. 仍然存在稀疏性问题。

##### 2.1.2.2 点互信息

首先看如何解决高频词误导计算结果的问题。

最直接的想法：如果一个词与很多词共现，则降低其权重；反之，如果一个词只与个别词共现，则提高其权重。信息论中的**点互信息**(**Pointwise Mutual Information**,**PMI**)恰好能够做到这一点。对于词w和上下文c，其PMI为：

![2-2](2-2.jpg)

可以通过**最大似然估计(Maximum Likelihood Estimation,MLE)**,分别计算相关的概率值，具体公式为：

![](2-3.jpg)

所以公式2-2可写成如下：

![](2-6.jpg)

另外，当某个词与上下文之间共现次数较低时，可能会得到负的PMI值。考虑到这种情况下的PMI不太稳定(具有较大的方差)，在实际应用中通常采用**PPMI**( Postive PMI)的形式，即：

![](2-7.jpg)

词语共现频次表：

![](表2-1.jpg)

PMI代码实现：

```python 
import numpy as np

M = np.array([[0, 2, 1, 1, 1, 1, 1, 2, 1, 3],
              [2, 0, 1, 1, 1, 0, 0, 1, 1, 2],
              [1, 1, 0, 1, 1, 0, 0, 0, 0, 1],
              [1, 1, 1, 0, 1, 0, 0, 0, 0, 1],
              [1, 1, 1, 1, 0, 0, 0, 0, 0, 1],
              [1, 0, 0, 0, 0, 0, 1, 1, 0, 1],
              [1, 0, 0, 0, 0, 1, 0, 1, 0, 1],
              [2, 1, 0, 0, 0, 1, 1, 0, 1, 2],
              [1, 1, 0, 0, 0, 0, 0, 1, 0, 1],
              [3, 2, 1, 1, 1, 1, 1, 2, 1, 0]])

def pmi(M, positive=True):
    col_totals = M.sum(axis=0)
    row_totals = M.sum(axis=1)
    total = col_totals.sum()
    expected = np.outer(row_totals, col_totals) / total
    M = M / expected
    # Silence distracting warnings about log(0):
    with np.errstate(divide='ignore'):
        M = np.log(M)
    M[np.isinf(M)] = 0.0  # log(0) = 0
    if positive:
        M[M < 0] = 0.0
    return M

M_pmi = pmi(M)
np.set_printoptions(precision=2)
print(M_pmi)
```

除了PMI，还有很多其他方法可以达到类似的目的，如信息检索当中常用的TF-IDF等。

##### 2.1.2.3 奇异值分解

下面看如何解决共现频次无法反映词之间高阶关系的问题。其中**奇异值分解**(**Singular Value Decompositon**,SVD)是一种常见的做法。对共现矩阵M进行奇异值分解：

![](2-8.jpg)

在python的numpy.linalg库中内置了SVD函数，只需要输入共现矩阵，然后调用相应的函数即可，如：

U, s, Vh = np.linalg.svd(M_pmi)

svd代码实现：

```python 
U, s, Vh = np.linalg.svd(M_pmi)
import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
words = ["我", "喜欢", "自然", "语言", "处理", "爱", "深度", "学习", "机器", "。"]
for i in range(len(words)):
    plt.text(U[i, 0], U[i, 1], words[i])
plt.xlim(0, 0.6)
plt.ylim(-0.5, 0.6)
plt.savefig('svd.pdf')
plt.show()
```

![](图2-1.jpg)

#### 2.1.3 词嵌入表示

与词的分布式表示类似，词嵌入表示也使用一个连续、低维、稠密的向量来表示词，经常直接简称为**词向量**

如果目标任务的训练数据比较少，学习合适的词向量难度会比较大。

#### 2.1.4 文本的词袋表示

​	上面介绍了几种常见的词表示方法，那么如何通过词的表示构成更长文本的表示呢？在此介绍一种最简单的文本表示方法——**词袋**（**Bag-Of-Words，BOW**）表示。

所谓词袋表示，就是假设文本中的词语是没有顺序的集合，将文本中的全部词所对应的向量表示（既可以是独热表示，也可以是分布式表示或词向量）相加，即构成了文本的向量表示。如在使用独热表示时，文本向量表示的每一维恰好是相应的词在文本中出现的次数。
虽然这种文本表示的方法非常简单、直观，但是其**缺点**也非常明显：

1. 首先是没有考虑词的顺序信息，导致“张三打李四”和“李四打张三”，虽然含义不同，但是由于它们包含的词相同，即使词序不同，词袋表示的结果也是一样的；

 	2. 其次是无法融入上下文信息。比如要表示“不喜欢”，只能将两个词的向量相加，无法进行更细致的语义操作。当然，可以通过增加词表的方法加以解决，比如引入二元词（Bigram）词表，将“不+喜欢”等作为“词”，然后同时学习二元词的词向量表示。这种方法既能部分解决否定词的问题，也能部分解决局部词序的问题，但是随着词表的增大，会引入更严重的数据稀疏问题。深度学习技术的引入为解决这些问题提供了更好的方案，本书后续章节将进行更详细的介绍。

### 2.2 自然语言处理任务

- 语言模型
- 基础任务
- 应用任务

#### 2.2.1 语言模型

##### 2.2.2.1 N元语言模型

语言模型的基本任务是在给定词序列w1w2··· wt−1的条件下，对下一时刻t可能出现的词wt的条件概率P （wt|w1w2··· wt−1）进行估计。一般地，把w1w2··· wt−1称为wt的**历史**。例如，对于历史“我喜欢”，希望得到下一个词为“读书”的概率，即：P（读书|我喜欢）。在给定一个语料库时，该条件概率可以理解为当语料中出现“我喜欢”时，有多少次下一个词为“读书”，然后通过最大似然估计进行计算：

![](2-9.jpg)



其中，C(.)表示相应词序列在该语料库中出现的次数(也称为频次)。

通过以上的条件概率，可以进一步计算一个句子出现的概率，即相应单词序列的联合概率P(w1w2...wl)，式中l为序列长度，可以利用链式法则对该式进行分解，从而转化为条件概率的计算问题：

![](2-10.jpg)

式中，wi：j表示由位置i到j的子串wiwi+1··· wj。
然而，随着句子长度的增加，w1：i−1出现的次数会越来越少，甚至从未出现过，那么P （wi|w1：i−1）则很可能为0，此时对于概率估计就没有意义了。为了解决该问题，可以假设“下一个词出现的概率只依赖于它前面n−1个词”，即：

![](2-11.jpg)

该假设被称为**马尔可夫假设（Markov Assumption）**。满足这种假设的模型，被称为**N元语法**或N元文法（N-gram）模型。特别地，当n=1时，下一个词的出现独立于其历史，相应的一元语法通常记作**unigram**。当n=2时，下一个词只依赖于前1个词，对应的二元语法记作**bigram**。二元语法模型也被称为一阶马尔可夫链（Markov Chain）。类似的，三元语法假设（n=3）也被称为二阶马尔可夫假设，相应的三元语法记作**trigram**。n的取值越大，考虑的历史越完整。在unigram模型中，由于词与词之间相互独立，因此它是与语序无关的。
以bigram模型为例，式（2-10）可转换为：

![](2-12.jpg)

为了使 P （wi|wi−1） 对于 i=1 有意义，可在句子的开头增加一个句首标记＜BOS＞（Begin Of Sentence），并设w0=＜BOS＞。同时，也可以在句子的结尾增加一个句尾标记＜EOS＞（End Of Sentence），设wl+1=＜EOS＞。

##### 2.2.2.2 平滑

虽然马尔可夫假设（下一个词出现的概率只依赖于它前面n−1个词）降低了句子概率为0的可能性，但是当n比较大或者测试句子中含有**未登录词**（Out-Of-Vocabulary，**OOV**）时，仍然会出现“零概率”问题。由于数据的稀疏性，训练数据很难覆盖测试数据中所有可能出现的N-gram，但这并不意味着这些N-gram出现的概率为0。为了避免该问题，需要使用平滑（Smoothing）技术调整概率估计的结果。本节将介绍一种最基本，也最简单的平滑算法——折扣法。

**折扣法（Discounting）**平滑的基本思想是“损有余而补不足”，即从频繁出现的N-gram中匀出一部分概率并分配给低频次（含零频次）的N-gram，从而使得整体概率分布趋于均匀。加1平滑（Add-one Discounting）是一种典型的折扣法，也被称为**拉普拉斯平滑**（Laplace Smoothing），它假设所有N-gram的频次比实际出现的频次多一次。例如，对于unigram模型来说，平滑之后的概率可由以下公式计算：

![](2-13.jpg)

式中，|V|是词表大小。所有未登录词可以映射为一个区别于其他已知词汇的独立标记，如＜UNK＞。
相应的，对于bigram模型，则有：

![](2-14.jpg)

在实际应用中，尤其当训练数据较小时，加1平滑将对低频次或零频次事件给出过高的概率估计。一种自然的扩展是加δ平滑。在加δ平滑中，假设所有事件的频次比实际出现的频次多δ次，其中0≤δ≤1。
以bigram语言模型为例，使用加δ平滑之后的条件概率为：

![](2-15.jpg)

关于超参数δ的取值，需要用到开发集数据。根据开发集上的困惑度对不同δ取值下的语言模型进行评价，最终将最优的δ用于测试集。

由于引入了马尔可夫假设，导致**N元语言模型无法对长度超过N 的长距离词语依赖关系进行建模**，如果将 N 扩大，又会带来更严重的数据稀疏问题，同时还会急剧增加模型的参数量（N-gram数目），为存储和计算都带来极大的挑战。5.1节将要介绍的神经网络语言模型可以较好地解决N元语言模型的这些缺陷。



##### 2.2.2.3 语言模型性能评价

如何衡量一个语言模型的好坏呢？目前最为常用的是基于困惑度（Perplexity，PPL）的“内部评价”方式。

为了进行内部评价，首先将数据划分为不相交的两个集合，分别称为**训练集**和**测试集**，其中用于估计语言模型的参数。由该模型计算出的测试集的概率则反映了模型在测试集上的泛化能力。
假设测试集$D^test=w1w2...w_N$（每个句子的开始和结束分布增加＜BOS＞与＜EOS＞标记），那么测试集的概率为：

![](2-16.jpg)

困惑度则为模型分配给测试集中每一个词的概率的几何平均值的倒数：

![](2-17.jpg)

例如，对于bigram模型而言：

![](2-18.jpg)

在实际计算过程中，考虑到多个概率的连乘可能带来浮点数下溢的问题，通常需要将式（2-18）转化为对数和的形式：

![](2-19.jpg)

困惑度越小，意味着单词序列的概率越大，也意味着模型能够更好地解释测试集中的数据。需要注意的是，困惑度越低的语言模型并不总是能在外部任务上取得更好的性能指标，但是两者之间通常呈现出一定的正相关性。因此，困惑度可以作为一种快速评价语言模型性能的指标，而在将其应用于下游任务时，仍然需要根据其在具体任务上的表现进行评价。

#### 2.2.2 自然语言处理基础任务

常见基础任务：词法分析(分词、词性标注)、句法分析和语义分析。

##### 2.2.2.1 中文分词

最简单的分词算法：正向最大匹配(Forward Maximum Matching,FMM)分词算法，即：

从前向后扫描句子中的字符串，尽量找到词典中较长的单词作为分词的结果。具体代码如下：

```python 
# Defined in Section 2.2.2
# 这也是一个分句的思路
from os import name
from typing import Mapping

def load_dict():
    f = open("lexicon.txt")
    lexicon = set()
    max_len = 0
    for line in f:
        word = line.strip()
        lexicon.add(word)
        if len(word) > max_len:
            max_len = len(word)
    f.close()

    return lexicon, max_len

def fmm_word_seg(sentence, lexicon, max_len):
    begin = 0
    end = min(begin + max_len, len(sentence))
    words = []
    while begin < end:
        word = sentence[begin:end]
        if word in lexicon or end - begin == 1:
            words.append(word)
            begin = end
            end = min(begin + max_len, len(sentence))
        else:
            end -= 1
    return words

if __name__ == '__main__':
    lexicon, max_len = load_dict()
    words = fmm_word_seg(input("请输入句子："), lexicon, max_len)

    for word in words:
        print(word,) 
'''
请输入句子：我爱你，你喜欢我 吗              
我
爱
你
，
你
喜欢
我
吗
'''        

```

FMM的缺点：

1. 倾向于切分出较长的词
2. OOV问题

##### 2.2.2.2 子词切分

​	一般认为，以英语为代表的印欧语系的语言，词语之间通常已有分隔符（空格等）进行切分，无须再进行额外的分词处理。然而，由于这些语言往往具有复杂的词形变化，如果仅以天然的分隔符进行切分，不但会造成一定的数据稀疏问题，还会导致由于词表过大而降低处理速度。如“computer”“computers”“computing”等，虽然它们语义相近，但是被认为是截然不同的单词。传统的处理方法是根据语言学规则，引入**词形还原**（Lemmatization）或者**词干提取**（Stemming）等任务，提取出单词的词根，从而在一定程度上克服数据稀疏问题。其中，词形还原指的是**将变形的词语转换为原形**，如将“computing”还原为“compute”；而词干提取则是**将前缀、后缀等去掉，保留词干（Stem）**，如“computing”的词干为“comput”，可见，词干提取的结果可能不是一个完整的单词。
词形还原或词干提取虽然在一定程度上解决了数据稀疏问题，但是需要人工撰写大量的规则，这种基于规则的方法既不容易扩展到新的领域，也不容易扩展到新的语言上。因此，基于统计的无监督子词（Subword）切分任务应运而生，并在现代的预训练模型中使用。

​	所谓**子词切分**，就是**将一个单词切分为若干连续的片段**。目前有多种常用的子词切分算法，它们的方法大同小异，基本的原理都是使用尽量长且频次高的子词对单词进行切分。此处重点介绍常用的字节对编码（Byte Pair Encoding，**BPE**）**算法**。
首先，BPE通过算法2.1构造子词词表。

![](BPE算法.jpg)
下面，通过一个例子说明如何构造子词词表。具体见课本。

##### 2.2.2.3 词性标注

词性是词语在句子中扮演的语法角色，也被称为**词类（Part-Of-Speech，POS）**。例如，表示抽象或具体事物名字（如“计算机”）的词被归为名词，而表示动作（如“打”）、状态（如“存在”）的词被归为动词。词性可为句法分析、语义理解等提供帮助。
**词性标注（POS Tagging）**任务是指**给定一个句子，输出句子中每个词相应的词性**。

词性标注的主要难点在于歧义性，即一个词在不同的上下文中可能有不同的词性。例如，上例中的“下”，既可以表示动词，也可以表示方位词。因此，需要结合上下文确定词在句子中的具体词性。

##### 2.2.2.4 句法分析

句法分析（Syntactic Parsing）的主要目标是**给定一个句子，分析句子的句法成分信息，例如主谓宾定状补等成分**。最终的目标是将词序列表示的句子转换成树状结构，从而有助于更准确地理解句子的含义，并辅助下游自然语言处理任务。

##### 2.2.2.5 语义分析

1. 词义消歧
2. 语义依存图
3. 概念语义图

#### 2.2.3 自然语言处理应用任务

##### 2.2.3.1 信息抽取

1. 命名实体识别
2. 关系抽取
3. 事件抽取

##### 2.2.3.2 情感分析

1. 情感分类
2. 情感信息抽取

##### 2.2.3.3 问答系统

1.  检索式问答系统
2. 知识库问答系统
3. 常问问题集问答系统
4. 阅读理解式问题系统

##### 2.2.3.4 机器翻译

神经网络机器翻译：nmt

##### 2.2.3.5 对话系统

1. 任务型对话系统
2. 开放域对话系统

### 2.3 基本问题

文本分类、结构预测、序列到序列问题

#### 2.3.1 文本分类问题

#### 2.3.2 结构预测问题

##### 2.3.2.1 序列标注

##### 2.3.2.2 序列分割

##### 2.3.2.3 图结构生成

#### 2.3.3 序列到序列问题

机器翻译是典型的代表序列到序列问题

### 2.4 评价指标

准确率是最简单、直观的评价指标，经常用于文本分类等问题，其计算公式如下：

![](2-20.jpg)

词性标注等序列标注问题也可以采用准确率进行评价，即：

![](2-21.jpg)

但是，并非全部的序列标注问题都可以采用准确率进行评价，如在将分词、命名实体识别等序列分割问题转化为序列标注问题后，就不应该使用准确率进行评价。以命名实体识别为例，如果采用按词计算的准确率，则很多非命名实体（相应词对应的类别为O）也被计入准确率的计算之中。另外，如果错标了部分词，那么命名实体识别结果就是错误的，但是按照词准确率计算的话，仍然有部分词被认为分类正确了。如表2-10中的例子所示，按照词（此处为汉字）计算，在8个输入词中，仅仅预测错了1个（三），则准确率为7/8=0.875，这显然是不合理的。分词等其他序列分割问题的评价也存在类似的问题。

![](表2-10.jpg)

表2-10 命名实体识别评价示例

那么，如何更合理地评价序列分割问题的性能呢？这就需要引入F值（F-Mea-sure或F-Score）评价指标，其是精确率（Precision）和召回率（Recall）的加权调和平均，具体公式为：

![](2-22.jpg)

式中，β是加权调和参数；P 是精确率；R是召回率。当β=1时，即精确率和召回率的权重相同，此时F值又称为F1值，具体公式为：

![](2-23.jpg)

在命名实体识别问题中，精确率和召回率的定义分别为：

![](2-24.jpg)

仍以表2-10中的示例为例，其中，“正确识别的命名实体数目”为1（“哈尔滨”），“识别出的命名实体总数”为2（“张”和“哈尔滨”），“测试文本中命名实体的总数”为2（“张三”和“哈尔滨”），那么此时精确率和召回率皆为1/2=0.5，最终的F1=0.5。与基于词计算的准确率（0.875）相比，该值更为合理了。

理解了准确率和F值两种评价指标的区别和联系后，就可以很容易地为一个自然语言处理任务选择合适的评价指标。

**BLEU**值是最常用的机器翻译自动评价指标，其计算方法是**统计机器译文与参考译文（可以不止一个）中 N-gram 匹配的数目占机器译文中所有 N-gram 总数的比率，即N-gram的精确率**。其中N 的取值不易过大，也不易过小。过大的N 会导致机器译文与参考译文中共现的N-gram过少，而过小的N 会无法衡量机器译文中词语的顺序信息，所以N一般最大取4。

## 第三章 基础工具集与常用数据集

本章内容：

1. 介绍2种常用的NLP基础工具集：英文处理工具NLTK和中文处理工具LTP
2. 深度学习框架pytorch

### 3.1 NLTK工具集

NLTK（Natural Language Toolkit）是一个 Python 模块，提供了多种**语料库（Corpora）**和**词典（Lexicon）**资源，如WordNet[1]等，以及一系列基本的自然语言处理工具集，包括：**分句、标记解析（Tokenization）、词干提取（Stemming）、词性标注（POS Tagging）和句法分析（Syntactic Parsing）**等，是对英文文本数据进行处理的常用工具。
为了使用NLTK，需要对其进行安装，可以直接使用pip包管理工具安装，具体方法为，首先进入操作系统的控制台，然后执行以下命令。

```python
pip install nltk
```

#### 3.1.1 常用语料库和词典资源

为了使用NLTK提供的语料库和词典资源，首先得进行下载：

```python
import nltk
nltk.download()
```

##### 3.1.1.1 停用词

在进行自然语言处理时，有一些词对于表达语言的含义并不重要，如英文中的冠词“a”“the”，介词“of”“to”等。因此，在对语言进行更深入的处理之前，可以将它们删除，从而加快处理的速度，减小模型的规模。这些词又被称为停用词（Stop words）。

NLTK提供了多种语言的停用词词表，可以通过下面语句引入停用词词表。然后，使用下面的语句查看一种语言的停用词词表（如英文）。

```python
from nltk.corpus import stopwords
print(stopwords.words('english'))
```

![](stopwords1.jpg)

![stopwords2](stopwords2.png)

##### 3.1.1.1.2 常用语料库

NLTK提供了多种语料库（文本数据集），如图书、电影评论和聊天记录等，它们可以被分为两类，即**未标注语料库**（又称**生语料库或生文本**，**Raw text**）和**人工标注语料库**（**Annotated corpus**）。下面就其中的典型语料库加以简要介绍，关于全部语料库的详细信息，可以通过NLTK的网站了解。

1. 未标注语料库。可以使用两种方式访问之前下载的语料库，第一种是直接访问语料库的原始文本文件（目录为下载数据时选择的存储目录）；另一种是调用NLTK提供的相应功能。例如，通过以下方式，可以获得古腾堡（Gutenberg）语料库（目录为：nltk_data/corpora/gutenberg）中简·奥斯汀（Jane Austen）所著的小说Emma原文。

![](gutenberg.jpg)

2. 人工标注语料库。人工标注的关于某项任务的结果。如在句子极性语料库（sentence_polarity）中，包含了10，662条来自电影领域的用户评论句子以及相应的极性信息（褒义或贬义）。通过以下命令，可以获得该语料库，其中，褒贬各5，331句（经过了小写转换、简单的标记解析等预处理后）。

   ```python
   from nltk.corpus import sentence_polarity
   ```

   `sentence_polarity`提供了基本的数据访问方法，

   `sentence_polarity.categories（）` 返回褒贬类别列表，即 ['neg'，'pos']；

   `sentence_polarity.words（）`返回语料库中全部单词的列表，如果调用时提供类别参数（categories=＂pos＂ 或 ＂neg＂），则会返回相应类别的全部单词列表；

   `sentence_polarity.sents（）`返回语料库中全部句子的列表，调用时同样可以提供类别参数。

   可以使用以上方法的组合，构造出一个大列表，其中每个元素为一个句子的单词列表及其对应的褒贬类别构成的元组。

3.常用词典

（1） WordNet。WordNet 是普林斯顿大学构建的英文语义词典（也称作辞典，Thesaurus），其主要特色是定义了同义词集合（Synset），每个同义词集合由具有相同意义的词义组成。此外，WordNet为每一个同义词集合提供了简短的释义（Gloss），同时，不同同义词集合之间还具有一定的语义关系。下面演示WordNet的简单使用示例。

![](wordnet.png)

NLTK提供的更多关于WordNet的功能请参考相应的官方文档。

（2） SentiWordNet。SentiWordNet（Sentiment WordNet）是基于WordNet标注的词语（更准确地说是同义词集合）情感倾向性词典，它为WordNet中每个同义词集合人工标注了三个情感值，依次是褒义、贬义和中性。通过该词典，可以实现一个简单的情感分析系统。仍然通过一个例子演示SentiWordNet的使用方法。

![](sentiwordnet.jpg)

#### 3.1.2 常用自然语言处理工具集

NLTK提供了多种常用的NLP基础工具，如分句、标记解析和词性标注等。

##### 3.1.2.1 分句

通常一个句子能够表达完整的语义信息，因此在进行更深入的自然语言处理之前，往往需要将较长的文档切分成若干句子，这一过程被称为**分句**。一般来讲，一个句子结尾具有明显的标志，如句号、问号和感叹号等，因此可以使用简单的规则进行分句。然而，往往存在大量的例外情况，如在英文中，句号除了可以作为句尾标志，还可以作为单词的一部分（如“Mr.”）。NLTK提供的分句功能可以较好地解决此问题。下面演示如何使用该功能。

![](sent_tokenize.jpg)

##### 3.1.2. 2 标记解析

一个句子是由**若干标记（`Token`）按顺序构成的**，其中**标记既可以是一个词，也可以是标点符号**等，这些标记是自然语言处理最基本的输入单元。**将句子分割为标记的过程**叫作**标记解析（`Tokenization`）**。英文中的单词之间通常使用空格进行分割，不过标点符号通常和前面的单词连在一起，因此标记解析的一项主要工作是将标点符号和前面的单词进行拆分。和分句一样，也无法使用简单的规则进行标记解析，仍以符号“.”为例，它既可作为句号，也可以作为标记的一部分，如不能简单地将“Mr.”分成两个标记。同样，NLTK提供了标记解析功能，也称作**标记解析器（`Tokenizer`）**。下面演示如何使用该功能。

![](word_tokenize.jpg)

##### 3.1.2. 3 词性标注

词性是词语所承担的语法功能类别，如名词、动词和形容词等，因此词性也被称为词类。很多词语往往具有多种词性，如“fire”，即可以作名词（“火”），也可以作动词（“开火”）。词性标注就是根据词语所处的上下文，确定其具体的词性。如在“`They sat by the fire.`”中，“fire”是名词，而在“`They fire a gun.`”中，“fire”就是动词。NLTK提供了**词性标注器（`POS Tagger`）**，下面演示其使用方法。

![](pos_tag.jpg)

其中，“fire”在第一个句子中被标注为名词（NN），在第二个句子中被标注为动词（VBP）。这里，词性标记采用宾州树库（`Penn Treebank`）的标注标准，NLTK提供了关于词性标记含义的查询功能，如下所示。

![](词性标注查询.jpg)

##### 3.1.2. 4 其他工具

除了以上介绍的分句、标记解析和词性标注，NLTK还提供了其他丰富的自然语言处理工具，包括**命名实体识别、组块分析（Chunking）和句法分析**等。
另外，除了NLTK，还有很多其他优秀的自然语言处理基础工具集可供使用，如斯坦福大学使用Java开发的CoreNLP、基于Python/Cython开发的**`spaCy`**等，它们的使用方法本书不再进行详细的介绍，感兴趣的读者可以自行查阅相关的参考资料。

### 3.2 LTP工具集

以上介绍的工具集主要用于英文的处理，而以中文为代表的汉藏语系与以英语为代表的印欧语系不同，一个显著的区别在于词语之间不存在明显的分隔符，句子一般是由一串连续的字符构成的，因此在处理中文时，需要使用更有针对性的分析工具。
语言技术平台（`Language Technology Platform，LTP`）[2]是哈尔滨工业大学社会计算与信息检索研究中心（HIT-SCIR）历时多年研发的一整套高效、高精度的中文自然语言处理开源基础技术平台。该平台集词法分析（分词、词性标注和命名实体识别）、句法分析（依存句法分析）和语义分析（语义角色标注和语义依存分析）等多项自然语言处理技术于一体。最新发布的LTP 4.0版本使用Python语言编写，采用预训练模型以及多任务学习机制，能够以较小的模型获得非常高的分析精度。
LTP的安装也非常简单，可以直接使用pip包管理工具，具体方法为，首先进入操作系统的控制台，然后执行以下命令。

```python 
pip install ltp
```

下面对LTP的使用方法进行简要的介绍。

#### 3.2.1 中文分词

如上所述，由于中文词语之间没有空格进行分割，而自然语言处理中通常以词为最小的处理单位，因此需要对中文进行分词处理。中文的分词与英文的标记解析功能类似，只是中文分词更强调识别句子中的词语信息，因此往往不被称为标记解析。另外，与标记解析相比，由于一个句子往往有多种可能的分词结果，因此分词任务的难度更高，精度也更低。使用 LTP 进行分词非常容易，具体示例如下。

![](ltp.png)

#### 3.2.2 其他中文自然语言处理功能

除了分词功能，LTP还提供了分句、词性标注、命名实体识别、依存句法分析和语义角色标注等功能。与NLTK类似，在此只演示如何使用LTP进行分句和词性标注，关于更多其他功能的使用方法，请参见LTP的官方文档。

![](ltp02.png)

### 3.3 Pytorch基础

安装：

```python
pip install torch
```

#### 3.3.1 张量的基本概念

所谓张量（Tensor），就是多维数组。当维度小于或等于2时，张量又有一些更熟悉的名字，例如，2维张量又被称为矩阵（Matrix），1维张量又被称为向量（Vector），而0维张量又被称为标量（Scalar），其实就是一个数值。使用张量，可以方便地存储各种各样的数据，如2维表格数据可以使用2维张量，即矩阵存储，而多张表格就可以使用3维张量表示和存储。一幅灰度图像（每个像素使用一个整数灰度值表示）也可以使用矩阵存储，而通常一副彩色图像（每个像素使用三个整数表示，分别代表红、绿、蓝的值）就可以使用3维张量表示和存储。
PyTorch提供了多种方式创建张量，如下所示。

![](torch01.png)

![torch02](torch02.jpg)

以上张量都存储在内存中，并使用CPU计算，若要早GPU中创建和计算张量，则需要显式的将其存入GPU中，具体可以采用下列方法之一(前提是本机已经配置了NVIDIA的GPU并且正确安装了相应的CUDA库)。

```python
torch.rand(2,3).cuda()
torch.rand(2,3).to("cuda")
torch.rand(2,3,device="cuda")
```

#### 3.3.2 张量的基本运算

![](torch03.png)

更多的运算方式可以通过torch中的函数实现，如向量点积（torch.dot）、矩阵相乘（torch.mm）、三角函数和各种数学函数等。具体示例如下。

![](torch04.jpg)

除了以上常用的数学运算，PyTorch还提供了更多的张量操作功能，如聚合操作（`Aggregation`）、拼接（`Concatenation`）操作、比较操作、随机采样和序列化等，详细的功能列表和使用方法可以参考PyTorch官方文档。
其中，当对张量进行聚合（如求平均、求和、最大值和最小值等）或拼接操作时，还涉及一个非常重要的概念，即维（Dim）或轴（Axis）。如对于一个张量，可以直接使用mean函数求其平均值。

![](torch05.jpg)

可见，直接调用mean函数获得的是全部6个数字的平均值。然而，有时需要对某一行或某一列求平均值，此时就需要使用维的概念。对于一个n维张量，其维分别是dim=0，dim=1，···，dim=n−1。在做张量的运算操作时，dim设定了哪个维，就会遍历这个维去做运算（也叫作沿着该维运算），其他维顺序不变。仍然是调用mean函数，当设定的维不同时，其结果也是不同的。

![](torch06.jpg)

以上演示了张量仅为2维（矩阵）的情况，当维度大于2时，其运算形式是什么样的呢？可以使用一个简单的规则描述，即“当dim=n时，则结果的n+1维发生变化，其余维不变”。如在上面的例子中，当dim=0时，则张量形状由原来的（2，3）变为（1，3）；当dim=1时，则张量形状由原来的（2，3）变为（2，1）。不过，细心的读者可能会发现，以上示例的运算结果形状并非（1，3）或（2，1）的矩阵，而分别是两个向量。为了使结果保持正确的维度，聚合操作还提供了keepdim参数，默认设置为False，需要显式地设为True。

![](torch07.jpg)

拼接（torch.cat）操作也是类似的，通过指定维，获得不同的拼接结果。如：

![](torch08.jpg)

![](torch09.jpg)

可见，拼接操作的运算规则也同样为“当dim=n时，则结果的n+1维发生变化，其余维不变”，如在上面的例子中，当dim=0时，则由原来两个形状为（2，3）的张量，拼接成一个（4，3）的张量；当dim=1时，则由原来两个形状为（2，3）的张量，拼接成一个形状为（2，6）的张量。
通过对以上多种操作的组合使用，就可以写出复杂的数学计算表达式。如对于数学表达式z=(x+y)×(y−2)
当x=2，y=3时，可以手动计算出z=5，当然也可以写一段简单的Python进行计算。

![](torch10.jpg)

那么，使用PyTorch如何计算z的值呢？其实PyTorch程序和Python非常类似，唯一不同之处在于数据使用张量进行保存。具体代码如下所示。

```python
x=torch.tensor([2.])
y=torch.tensor([3.])
z=(x+y)*(y-2)
print(z)
# tensor([5.])
```

使用不同GPU时，对三个较大的矩阵机芯相乘时，执行速度的对比：

![](torch11.jpg)

#### 3.3.3 自动微分

除了能显著提高执行速度，PyTorch还提供了自动计算梯度的功能（也叫自动微分），使得无须人工参与，即可自动计算一个函数关于一个变量在某一取值下的导数。通过该功能，就可以使用基于梯度的方法对参数（变量）进行优化（也叫学习或训练）。使用PyTorch计算梯度非常容易，仅需要执行tensor.backward（）函数，就可以通过反向传播算法（Back Propogation）自动完成。
需要注意的一点是，为了计算一个函数关于某一变量的导数，PyTorch要求显式地设置该变量（张量）是可求导的，否则默认不能对该变量求导。具体设置方法是在张量生成时，设置requires_grad=True。
因此，计算z=（x+y）×（y−2）的代码经过简单修改，就可以计算当x=2，y=3时，$dz/dx$ 和 $dz/dy$ 的值。

![](torch12.jpg)

#### 3.3.4 调整张量形状

​	参与运算的张量需要满足一定的形状，比如两个矩阵相乘，前一个矩阵的第二维应该和后一个矩阵的第一维相同。为了做到这一点，有时需要对张量的形状进行调整。PyTorch一共提供了4种调整张量形状的函数，分别为`view`、`reshape`、`transpose`和`permute`。下面分别加以介绍。
view函数的参数用于设置新的张量形状，因此需要保证张量总的元素个数不变。示例如下。

![](torch13.jpg)

进行view操作的张量要求是连续的（`Contiguous`），可以调用is_conuous函数判断一个张量是否为连续的。如果张量非连续，则需要先调用contiguous函数将其变为连续的，才能调用view函数。好在PyTorch提供了新的`reshape`函数，可以直接对非连续张量进行形状调整。除此之外，reshape函数与view函数功能一致。在此不再赘述。
transpose（转置）函数**用于交换张量中的两个维度**，参数分别为相应的维。如下所示。

![](torch14.jpg)

不过，transpose函数只能同时交换两个维度，若要交换更多的维度，需要多次调用该函数。更便捷的实现方式是直接调用permute函数，其需要提供全部的维度信息作为参数，即便有些维度无须交换也需要提供。示例如下所示。

![](torch15.jpg)

#### 3.3.5 广播机制

​	在上面介绍的张量运算中，都是假设两个参与运算的张量形状相同。在有些情况下，即使两个张量的形状不同，也可以通过**广播机制**（`Broadcasting Mechanism`）执行按元素计算。具体的执行规则是，首先，对其中一个或同时对两个张量的元素进行复制，使得这两个张量的形状相同；然后，在扩展之后的张量上再执行按元素运算。通常是沿着长度为1的维度进行扩展，下面通过一个具体的例子进行说明。

![](torch16.jpg)

​	生成两个张量，形状分别为（3，1）和（1，2），显然，它们不能直接执行按元素运算。因此，在执行按元素运算之前，需要将它们扩展（广播）为形状（3，2）的张量，具体扩展的方法为将x的第1列复制到第2列，将y的第1行复制到第2、3行。如下所示，可以直接进行加法运算，PyTorch会自动执行广播和按元素相加。

```python
print(x+y)
tensor([
    [5,6],
    [6,7],
    [7,8]
])
```

#### 3.3.6 索引与切片

​	与Python的列表类似，PyTorch中也可以对张量进行索引和切片操作，规则也与Python语言基本一致，即索引值是从0开始的，切片[m：n]的范围是从m开始，至n的前一个元素结束。与Python语言不同的是，PyTorch可以对张量的任意一个维度进行索引或切片。下面演示一些简单的示例。

![](torch17.png)

#### 3.3.7 降维与升维

- ​	**升维**，就是通过调用`torch.unsqueeze（input，dim，out=None）`函数，**对输入张量的dim位置插入维度1，并返回一个新的张量**。与索引相同，dim的值也可以为负数。

- ​	**降维**, ，使用`torch.squeeze（input，dim=None，out=None）`函数，**在不指定dim时，张量中形状为1的所有维都将被除去**。如输入形状为（A，1，B，1，C，1，D）的张量，那么输出形状就为（A，B，C，D）。当给定dim时，那么降维操作只在给定维度上。例如，输入形状为（A，1，B），squeeze（input，dim=0）函数将会保持张量不变，只有用squeeze（input，dim=1）函数时，形状才会变成（A，B）。下面给出调用示例。

![](torch18.png)

### 3.4 大规模预训练数据

#### 3.4.1 维基百科数据

`Wikipedia`

#### 3.4.2 原始数据的获取

![](维基百科.jpg)

预训练语言模型主要使用的是维基百科的正文内容，因此这里选择“`zhwiki-latest-pages-articles.xml.bz2`”，以下载最新快照的词条正文压缩包。以2020年10月23日的快照为例，该压缩包的大小约为1.95GB。由于后续进行处理时会直接对压缩包进行处理，这里不再进行解压缩操作。

#### 3.4.3 语料处理方法

##### 3.4.3.1 纯文本语料抽取

​	处理维基百科快照的方法相对比较成熟，这里以`WikiExtractor`为例进行介绍。WikiExtractor是一款基于Python的工具包，专门用于处理维基百科的快照。

```python
pip install wikiextractor
```

接下来，直接通过一行命令即可对维基百科的快照压缩包进行处理，去除其中的图片、表格、引用和列表等非常规文本信息，最终得到纯文本的语料。需要注意的是，这一部分的处理需要花费一定处理时间，视系统配置不同可能耗费几十分钟至数小时不等。

```python
python -m wikiextractor.WikiExtractor # 维基百科快照文件
```

对于`WikiExtractor`工具包的使用参数，可通过如下命令获取（普通用户使用默认参数即可）。

```python
python -m wikiextractor.WikiExtractor -h
```

处理完毕后，可以获得纯文本语料文件，其目录结构如下所示。

##### 3.4.3.2 中文繁简体转换

​	这里使用一款较为成熟的中文繁简体转换工具——OpenCC。OpenCC工具可将简体中文、繁体中文（其中包括中国香港地区、中国台湾地区使用的繁体）和日本新字体等中文进行互转。OpenCC工具同样可以通过pip命令安装。

```python
pip install opencc
```

安装完毕后，可以通过如下Python脚本进行中文繁简转换。

```python
python convert_t2s.py input_file > output_file
```

其中，转换脚本convert_t2s.py的内容如下所示。

```python
import sys
import opencc

converter = opencc.OpenCC("t2s.json")
f_in = open(sys.argv[1], "r")

for line in f_in.readlines():
    line = line.strip()
    line_t2s = converter.convert(line)
    print(line_t2s)
```

其中，配置文件t2s.json的内容如下：

![](t2s01.jpg)

![t2s02](t2s02.jpg)

#### 3.4.3.3 数据清洗

​	经过上述处理后，可以得到包含简体中文的纯文本语料。然而，在从维基百科快照里抽取纯文本数据的过程中可能因文本编码、损坏的HTML标签等问题导致纯文本中包含一些乱码或机器字符。因此，在最后需要通过一个简单的后处理操作对纯文本语料进行二次过滤，进一步提升预训练语料的质量。需要注意的是，这里仅处理语料中的一些明显错误，而对于一般类型的错误则不会处理（如标点不统一等问题），因为一般类型的错误在日常的文本中也会出现。这里的处理方式主要包括如下几类：
• 删除空的成对符号，例如“（）”“《》”“【】”“[]”等；
• 删除＜br＞等残留的HTML标签。需要注意的是，这里不删除以“＜doc id”和“＜/doc＞”为开始的行，因其表示文档的开始和结束，能为某些预训练语言模型的数据处理提供至关重要的信息；
• 删除不可见控制字符，避免意外导致数据处理中断。
所以，数据清洗将最大限度地保留自然文本的统计特征，对于其中的“对”与“错”，则交由模型来进行学习，而非通过人工进行过多干预。

通过如下脚本启动数据清洗过程：

```python
python wikidata_cleaning.py input_file > output_file
```

其中，数据清洗脚本 wikidata_cleaning.py如下：

```python
# Defined in Section 3.4.3

import sys
import re

def remove_empty_paired_punc(in_str):
    return in_str.replace('（）', '').replace('《》', '').replace('【】', '').replace('[]', '')
    
def remove_html_tags(in_str):
    html_pattern = re.compile(r'<[^>]+>', re.S)
    return html_pattern.sub('', in_str)

def remove_control_chars(in_str):
    control_chars = ''.join(map(chr, list(range(0, 32)) + list(range(127, 160))))
    control_chars = re.compile('[%s]' % re.escape(control_chars))
    return control_chars.sub('', in_str)

f_in = open(sys.argv[1], 'r')
for line in f_in.readlines():
    line = line.strip()
    if re.search(r'^(<doc id)|(</doc>)', line):
        print(line)
        continue
    line = remove_empty_paired_punc(line)
    line = remove_html_tags(line)
    line = remove_control_chars(line)
    print(line)

```

#### 3.4.4 Common Crawl数据

### 3.5 更多数据集

HiggingFace Datasets数据集：需要安装datasets包来获取。

```python
pip install datasets
```

下面通过一些示例，延时如何调用dadatsets提供的数据集以及评价方法。

![](datasets01.png)

datasets还提供了一些额外的函数，用于对数据进行处理或转换为PyTorch、TensorFlow等工具集能够处理的格式，具体调用方法见相应的使用文档。
datasets提供的评价方法调用示例如下。

![](datasets02.png)

![](datasets03.png)

最后，需要注意的是，除了能直接使用上述已有的评价方法，用户还可以增加自定义的评价方法，甚至提交到HuggingFace Hub上供他人使用。

## 第四章 自然语言处理中的神经网络基础

本章内容：

1. 多层感知器模型；
2. 卷积神经网络；
3. 循环神经网络；
4. 以Transformer为代表的自注意力模型。

### 4.1 多层感知器模型

#### 4.1.1 感知器

概念：

1.特征提取：将一个问题的原始输入(`Raw Input`)转换为输入向量`x`

2.参数学习：如何使用数值向量来表示文本；

#### 4.1.2 线性回归

输出的结果是连续的实数值

#### 4.1.3 Logistic回归

线性回归输出值的大小是任意的，有时要将其限制在一定的范围内，这就有了激活函数的概念。

#### 4.1.4 Softmax回归

sigmoid函数是softmax函数在处理二元分类问题时的一个特例。

#### 4.1.5 多层感知器

概念：`解决线性不可分问题`的一种解决方案，是堆叠多层线性分类器，并在中间层增加非线性激活函数。

#### 4.1.6 模型实现

##### 4.1.6.1 神经网络与激活函数

​	上面介绍了从简单的线性回归到复杂的多层感知器等多种神经网络模型，接下来介绍如何使用PyTorch实现这些模型。实际上，使用第3章介绍的PyTorch提供的基本张量存储及运算功能，就可以实现这些模型，但是这种实现方式不但难度高，而且容易出错。因此，PyTorch将常用的神经网络模型封装到了`torch.nn`包内，从而可以方便灵活地加以调用。如通过以下代码，就可以创建一个线性映射模型（也叫线性层）。

```python
from torch import nn
linear=nn.Linear(in_features,out_features)
```

代码中的 in_features 是输入特征的数目，out_features 是输出特征的数目。可以使用该线性映射层实现线性回归模型，只要将输出特征的数目设置为1 即可。当实际调用线性层时，可以一次性输入多个样例，一般叫作一个批次（`Batch`），并同时获得每个样例的输出。所以，如果输入张量的形状是 （`batch，in_features`），则输出张量的形状是（`batch，out_features`）。采用批次操作的好处是可以充分利用GPU等硬件的多核并行计算能力，大幅提高计算的效率。具体示例如下。

![](4-1.png)

Sigmoid、Softmax等各种激活函数包含在`torch.nn.functional`中，实现对输入按元素进行非线性运算，调用方式如下:

![](4-2.png)

##### 4.1.6.2 自定义神经网络模型

​	通过对上文介绍的神经网络层以及激活函数进行组合，就可以搭建更复杂的神经网络模型。在 PyTorch 中构建一个自定义神经网络模型非常简单，就是**从torch.nn中的Module类派生一个子类**，**并实现构造函数和forward函数**。其中，构造函数定义了模型所需的成员对象，如构成该模型的各层，并对其中的参数进行初始化等。而forward函数用来实现该模块的前向过程，即对输入进行逐层的处理，从而得到最终的输出结果。下面以多层感知器模型为例，介绍如何自定义一个神经网络模型，其代码如下。

```python
# Defined in Section 4.1.6
# MLP为例：

import torch
from torch import nn
from torch.nn import functional as F

class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_class):
        super(MLP, self).__init__()
        # 线性变换：输入层->隐含层
        self.linear1 = nn.Linear(input_dim, hidden_dim)
        # 使用ReLU激活函数
        self.activate = F.relu
        # 线性变换：隐含层->输出层
        self.linear2 = nn.Linear(hidden_dim, num_class)

    def forward(self, inputs):
        hidden = self.linear1(inputs)
        activation = self.activate(hidden)
        outputs = self.linear2(activation)
        probs = F.softmax(outputs, dim=1) # 获得每个输入属于某一类别的概率
        return probs

mlp = MLP(input_dim=4, hidden_dim=5, num_class=2)
inputs = torch.rand(3, 4) # 输入形状为(3, 4)的张量，其中3表示有3个输入，4表示每个输入的维度
probs = mlp(inputs) # 自动调用forward函数
print(probs) # 输出3个输入对应输出的概率
#tensor([[0.3200, 0.6800],
#        [0.3340, 0.6660],
#        [0.2869, 0.7131]], grad_fn=<SoftmaxBackward0>)
```



### 4.2 卷积神经网络

#### 4.2.1 模型结构

​	在多层感知器中，每层输入的各个元素都需要乘以一个独立的参数（权重），这一层又叫作**全连接层**（`Fully Connected Layer`）或**稠密层**（`Dense Layer`）。然而，对于某些类型的任务，这样做并不合适，如在图像识别任务中，如果对每个像素赋予独立的参数，一旦待识别物体的位置出现轻微移动，识别结果可能会发生较大的变化。在自然语言处理任务中也存在类似的问题，如对于情感分类任务，句子的情感极性往往由个别词或短语决定，而这些决定性的词或短语在句子中的位置并不固定，使用全连接层很难捕捉这种关键的局部信息。
​	为了解决以上问题，一个非常直接的想法是使用一个小的稠密层提取这些局部特征，如图像中固定大小的像素区域、文本中词的N-gram等。为了解决关键信息位置不固定的问题，可以依次扫描输入的每个区域，该操作又被称为**卷积**（`Con-volution`）操作。其中，每个小的、用于提取局部特征的稠密层又被称为**卷积核**（`Kernel`）或者**滤波器**（`Filter`）。
​	卷积操作输出的结果还可以进行进一步聚合，这一过程被称为**池化**（`Pooling`）操作。常用的池化操作有最大池化、平均池化和加和池化等。以最大池化为例，其含义是<u>仅保留最有意义的局部特征</u>。如在情感分类任务中，保留的是句子中对于分类最关键的N-gram信息。**池化操作的好处**是`可以解决样本的输入大小不一致的问题`，如对于情感分类，有的句子比较长，有的句子比较短，因此不同句子包含的N-gram数目并不相同，导致抽取的局部特征个数也不相同，然而经过池化操作后，可以保证最终输出相同个数的特征。
​	然而，如果仅使用一个卷积核，则只能提取单一类型的局部特征。而在实际问题中，往往需要提取很多种局部特征，如在情感分类中不同的情感词或者词组等。因此，在进行卷积操作时，可以使用多个卷积核提取不同种类的局部特征。卷积核的构造方式大致有两种，一种是<u>使用不同组的参数，并且使用不同的初始化参数</u>，获得不同的卷积核；<u>另一种是提取不同尺度的局部特征</u>，如在情感分类中提取不同大小的N-gram。
​	既然多个卷积核输出多个特征，那么这些特征对于最终分类结果的判断，到底哪些比较重要，哪些不重要呢？其实只要再经过一个全连接的分类层就可以做出最终的决策。最后，还可以将多个卷积层加池化层堆叠起来，形成更深层的网络，这些网络统称为**卷积神经网络（`Convolutional Neural Network，CNN`）**。
​	图4-7给出了一个卷积神经网络的示意图，用于对输入的句子分类。其中，输入为“我喜欢自然语言处理。”6个词。根据2.1.3节介绍的方法，首先将每个词映射为一个词向量，此处假设每个词向量的维度为5（图中输入层的每列表示一个词向量，每个方框表示向量的一个元素）。然后，分别使用4个卷积核对输入进行局部特征提取，其中前两个卷积核的宽度（N-gram中N的大小）为4（黄色和蓝色），后两个卷积核的宽度为3（绿色和红色），卷积操作每次滑动1个词，则每个卷积核的输出长度为`L−N+1`，<u>其中L为单词的个数，N为卷积核的宽度</u>，简单计算可以得到前两组卷积核的输出长度为3，后两组卷积核的输出长度为4。接下来，经过全序列的最大池化操作，将不同卷积核的输出分别聚合为1个输出，并拼接为一个特征向量，最终经过全连接层分类。
​	上面这种沿单一方向滑动的卷积操作又叫作一维卷积，适用于自然语言等序列数据。而对于图像等数据，由于卷积核不但需要横向滑动，还需要纵向滑动，此类卷积叫作二维卷积。类似的还有三维卷积，由于它们在自然语言处理中并不常用，因此本书不进行过多的介绍，感兴趣的读者请参考相关的深度学习书籍。

![](4-3.jpg)

与4.1.5节介绍的多层感知器模型类似，卷积神经网络中的信息也是从输入层经过隐含层，然后传递给输出层，按照一个方向流动，因此它们都被称为**前馈神经网络（`Feed-Forward Network，FFN`）**。

#### 4.2.2 模型实现

PyTorch的torch.nn包中使用Conv1d、Conv2d或Conv3d类实现卷积层，它们分别表示一维卷积、二维卷积和三维卷积。此处仅介绍自然语言处理中常用的一维卷积（Conv1d），其构造函数至少需要提供三个参数：in_channels为输入通道的个数，在输入层对应词向量的维度；out_channels为输出通道的个数，对应卷积核的个数；kernel_size为每个卷积核的宽度。当调用该Conv1d对象时，输入数据形状为（batch，in_channels，seq_len），输出数据形状为（batch，out_channels，seq_len），其中在输入数据和输出数据中，seq_len分别<u>表示输入的序列长度和输出的序列长度</u>。与图4-7相对应的网络构建代码如下。

![](4-4.png)

接下来需要调用torch.nn包中定义的池化层类，主要有最大池化、平均池化等。与卷积层类似，各种池化方法也分为一维、二维和三维三种。例如MaxPool1d是一维最大池化，其构造函数至少需要提供一个参数——kernel_size，即池化层核的大小，也就是对多大范围内的输入进行聚合。如果对整个输入序列进行池化，则其大小应为卷积层输出的序列长度。

![](4-5.png)

![4-6](4-6.jpg)

除了使用池化层对象实现池化，PyTorch还在`torch.nn.functional`中实现了池化函数，如max_pool1d等，即无须定义一个池化层对象，就可以直接调用池化功能。这两种实现方式基本一致，一个显著的区别在于使用池化函数实现无须事先指定池化层核的大小，只要在调用时提供即可。当处理不定长度的序列时，此种实现方式更加适合，具体示例如下。

![4-7](4-7.png)

由于outputs_pool1和outputs_pool2是两个独立的张量，为了进行下一步操作，还需要调用 torch.cat 函数将它们拼接起来。在此之前，还需要<u>调用`squeeze函数`将最后一个为1的维度删除，即将2行1列的矩阵变为1个向量。</u>

![4-8](4-8.jpg)

![4-9](4-9.jpg)

池化后，再连接一个FC层，实现分类功能。

![](4-10.jpg)

### 4.3 循环神经网络

主要讲RNN和LSTM

#### 4.3.1 模型结构

​	循环神经网络指的是网络的隐含层输出又作为其自身的输入，其结构如图4-8所示，图中Wxh、bxh，Whh、bhh和Why、bhy分别是输入层到隐含层、隐含层到隐含层和隐含层到输出层的参数。当实际使用循环神经网络时，需要设定一个有限的循环次数，将其展开后相当于堆叠多个共享隐含层参数的前馈神经网络。

![](4-11.jpg)

​										图4-8 循环神经网络示意图
​	当使用循环神经网络处理一个序列输入时，需要将循环神经网络按输入时刻展开，然后将序列中的每个输入依次对应到网络不同时刻的输入上，并将当前时刻网络隐含层的输出也作为下一时刻的输入。图4-9展示了循环神经网络处理序列输入的示意图，其中序列的长度为n。按时刻展开的循环神经网络可以使用如下公式描述：

![](4-13.jpg)

式中，tanh是激活函数，其形状与Sigmoid函数类似，只不过值域在−1到+1之间；t是输入序列的当前时刻，其隐含层ht不但与当前的输入xt有关，而且与上一时刻的隐含层ht−1有关，这实际上是一种递归形式的定义。每个时刻的输入经过层层递归，对最终的输出产生一定的影响，每个时刻的隐含层ht承载了1 ∼ t时刻的全部输入信息，因此循环神经网络中的隐含层也被称作**记忆（`Memory`）单元。**

![](4-12.jpg)

​								图4-9 循环神经网络处理序列输入示意图
<u>以上循环神经网络在最后时刻产生输出结果，此时适用于处理文本分类等问题。除此之外，如图4-10所示，还可以在每个时刻产生一个输出结果，这种结构适用于处理自然语言处理中常见</u>的**序列标注**（`Sequence Labeling`）问题（见2.3.2节），如词性标注、命名实体识别，甚至分词等。

![](4-14.jpg)

​					图4-10 循环神经网络用于处理序列标注问题的示意图


#### 4.3.2 LSTM

​	在原始的循环神经网络中，信息是通过多个隐含层逐层传递到输出层的。直观上，<u>这会导致信息的损失</u>；更本质地，这会<u>使得网络参数难以优化</u>。长短时记忆网络（LSTM）可以较好地解决该问题。其实不至解决了上面这俩问题，还解决了梯度爆炸和过拟合问题。
长短时记忆网络首先将式（4-13）的隐含层更新方式修改为：

![](4-15.jpg)

这样做的一个直观好处是直接将hk与ht（k＜t）进行了连接，跨过了中间的t−k层，从而减小了网络的层数，使得网络更容易被优化。其证明方式也比较简单，即：ht=ht−1+ut=ht−2+ut−1+ut=hk+uk+1+uk+2+···+ut−1+ut。
不过式（4-16）简单地将旧状态ht−1和新状态ut进行相加，这种更新方式过于粗糙，并<u>没有考虑两种状态对ht贡献的大小</u>。为解决这一问题，可以通过前一时刻的隐含层和当前输入计算一个系数，并以此系数对两个状态加权求和，具体公式为：

![](4-16.jpg)

式中，σ表示Sigmoid函数，其输出恰好介于0到1之间，可作为加权求和的系数；⊙表示Hardamard乘积，即按张量对应元素进行相乘；ft被称作**遗忘门**（`Forget gate`），因为如果其较小时，旧状态ht−1对当前状态的贡献也较小，也就是将过去的信息都遗忘了。
然而，这种加权的方式有一个问题，就是旧状态ht−1和新状态ut的贡献是互斥的，也就是如果ft较小，则1−ft就会较大，反之亦然。但是，<u>这两种状态对当前状态的贡献有可能都比较大或者比较小</u>，因此需要使用独立的系数分别控制。因此，引入新的系数以及新的加权方式，即：

![](4-17.jpg)

式中，新的系数it用于控制输入状态ut对当前状态的贡献，因此又被称作**输入门**（`Input gate`）。
类似地，还可以对输出增加门控机制，即**输出门**（`Output gate`）：

![](4-18.jpg)

式中，ct又被称为**记忆细胞**（`Memory cell`），即存储（记忆）了截至当前时刻的重要信息。与原始的循环神经网络一样，既可以使用hn预测最终的输出结果，又可以使用ht预测每个时刻的输出结果。
​	无论是传统的循环神经网络还是LSTM，信息流动都是单向的，在一些应用中这并不合适，如对于词性标注任务，一个词的词性不但与其前面的单词及其自身有关，还与其后面的单词有关，但是传统的循环神经网络并不能利用某一时刻后面的信息。为了解决该问题，可以使用**双向循环神经网络**或**双向`LSTM`**，简称`Bi-RNN`或`Bi-LSTM`，其中Bi代表`Bidirectional`。**其思想**: 是<u>将同一个输入序列分别接入向前和向后两个循环神经网络中，然后再将两个循环神经网络的隐含层拼接在一起，共同接入输出层进行预测。</u>双向循环神经网络结构如图4-11所示。

![](4-20.jpg)

​									图4-11 双向循环神经网络结构
另一类对循环神经网络的改进方式是将多个网络堆叠起来，形成**堆叠循环神经网络**（`Stacked RNN`），如图4-12所示。此外，还可以在堆叠循环神经网络的每一层加入一个反向循环神经网络，构成更复杂的**堆叠双向循环神经网络**。

![](4-21.jpg)

​								图4-12 堆叠循环神经网络示意图

#### 4.3.3 模型实现

​	循环神经网络在PyTorch的torch.nn包中也有相应的实现，即`RNN类`。其构造函数至少需要提供两个参数：input_size表示每个时刻输入的大小，hidden_size表示隐含层的大小。另外，根据习惯，通常将batch_first设为True（其默认值为False），即输入和输出的第1维代表批次的大小（即一次同时处理序列的数目）。当调用该RNN对象时，输入数据形状为（`batch，seq_len，input_size`），输出数据有两个，分别为隐含层序列和最后一个时刻的隐含层，它们的形状分别为（`batch，seq_len，hidden_size`）和（`1，batch，hidden_size`）。具体的示例代码如下。

![](4-22.png)

​	当初始化RNN时，还可通过设置其他参数修改网络的结构，如bidirectional=True（双向RNN，默认为False）、num_layers（堆叠的循环神经网络层数，默认为1）等。
​	torch.nn包中还提供了LSTM类，其初始化的参数以及输入数据与RNN相同，不同之处在于其输出数据除了最后一个时刻的隐含层hn，还输出了最后一个时刻的记忆细胞cn，代码示例如下。

![](4-23.png)

#### 4.3.4 基于循环神经网络的序列到序列模型

​	除了能够处理分类问题和序列标注问题，循环神经网络另一个强大的功能是能够处理序列到序列的理解和生成问题，相应的模型被称为**序列到序列模型**，也被称为**编码器--解码器模型**。**序列到序列模型**指的<u>是首先对一个序列（如一个自然语言句子）编码，然后再对其解码，即生成一个新的序列</u>。很多自然语言处理问题都可以看作序列到序列问题，如**机器翻译**，即首先对源语言的句子编码，然后生成相应的目标语言翻译。
​	图4-13展示了一个基于序列到序列模型进行机器翻译的示例。首先编码器使用循环神经网络对源语言句子编码，然后以最后一个单词对应的隐含层作为初始，再调用解码器（另一个循环神经网络）逐词生成目标语言的句子。图中的BOS表示句子起始标记。

![](4-24.jpg)

​									图4-13 序列到序列模型
<u>基于循环神经网络的序列到序列模型有一个基本假设，就是原始序列的最后一个隐含状态（一个向量）包含了该序列的全部信息</u>。然而，该假设显然不合理，尤其是当序列比较长时，要做到这一点就更困难。为了解决该问题，注意力模型应运而生。

### 4.4 注意力模型

为什么会存在注意力模型？主要为了解决什么问题？

#### 4.4.1 注意力机制

​	为了<u>解决序列到序列模型记忆长序列能力不足的问题</u>，一个非常直观的想法是，<u>当要生成一个目标语言单词时，不光考虑前一个时刻的状态和已经生成的单词，还考虑当前要生成的单词和源语言句子中的哪些单词更相关，即更关注源语言的哪些词</u>，这种做法就叫作**注意力机制（`Attention mechanism`）**。图4-14给出了一个示例，假设模型已经生成单词“我”后，要生成下一个单词，显然和源语言句子中的“love”关系最大，因此将源语言句子中“love”对应的状态乘以一个较大的权重，如0.6，而其余词的权重则较小，最终将源语言句子中每个单词对应的状态加权求和，并用作新状态更新的一个额外输入。
注意力权重的计算公式为：

![](4-25.jpg)

式中，hs表示源序列中s时刻的状态；ht−1表示目标序列中前一个时刻的状态；attn是注意力计算公式，即通过两个输入状态的向量，计算一个源序列s时刻的注意力分数；，其中 L为源序列的长度；最后对整个源序列每个时刻的注意力分数使用Softmax函数进行归一化，获得最终的注意力权重αs。

![](4-26.jpg)

​						图4-14 基于注意力机制的序列到序列模型示例
注意力公式attn的计算方式有多种，如：

![](4-27.jpg)

通过引入注意力机制，使得基于循环神经网络的序列到序列模型的准确率有了大幅度的提高。


#### 4.4.2 自注意力模型

​	受注意力机制的启发，当要表示序列中某一时刻的状态时，可以通过该状态与其他时刻状态之间的相关性（注意力）计算，即所谓的“<u>观其伴、知其义</u>”，这又被称作**自注意力机制（`Self-attention`）**。
具体地，假设输入为n个向量组成的序列x1， x2，···， xn，输出为每个向量对应的新的向量表示y1， y2，···， yn，其中所有向量的大小均为d。那么，yi的计算公式为：

![](4-2702.jpg)

式中，j 是整个序列的索引值；αij是 xi与 xj之间的注意力（权重），其通过式（4-26）中的attn函数计算，然后再经过Softmax函数进行归一化后获得。直观上的含义是如果xi与xj越相关，则它们计算的注意力值就越大，那么xj对xi对应的新的表示yi的贡献就越大。
​	通过自注意力机制，可以直接计算两个距离较远的时刻之间的关系。而在循环神经网络中，由于信息是沿着时刻逐层传递的，因此当两个相关性较大的时刻距离较远时，会产生较大的信息损失。虽然引入了门控机制模型，如LSTM等，可以部分解决这种长距离依赖问题，但是治标不治本。因此，基于自注意力机制的自注意力模型已经逐步取代循环神经网络，成为自然语言处理的标准模型。

#### 4.4.3 Transformer

然而，要想真正取代循环神经网络，自注意力模型还需要解决如下问题：
1）、<u>在计算自注意力时，没有考虑输入的位置信息，因此无法对序列进行建模；</u>
2）、<u>输入向量xi同时承担了三种角色，即计算注意力权重时的两个向量以及被加权的向量，导致其不容易学习</u>；
3）、<u>只考虑了两个输入序列单元之间的关系，无法建模多个输入序列单元之间更复杂的关系</u>

4）、<u>自注意力计算结果互斥，无法同时关注多个输入。</u>
​	下面分别就这些问题给出相应的解决方案，融合了以下方案的自注意力模型拥有一个非常炫酷的名字——`Transformer`。这个单词并不容易翻译，从本意上讲，其是将一个向量序列变换成另一个向量序列，所以可以翻译成“变换器”或“转换器”。其还有另一个含义是“变压器”，也就是对电压进行变换，所以翻译成变压器也比较形象。当然，还有一个更有趣的翻译是“变形金刚”，这一翻译不但体现了其能变换的特性，还寓意着该模型如同变形金刚一样强大。目前，Transformer还没有一个翻译的共识，绝大部分人更愿意使用其英文名。
-------------下面来解决上面这四个问题：

##### 4.4.3.1 融入位置信息

​	位置信息对于序列的表示至关重要，原始的自注意力模型没有考虑输入向量的位置信息，导致其与词袋模型类似，两个句子只要包含的词相同，即使顺序不同，它们的表示也完全相同。为了解决这一问题，需要为序列中每个输入的向量引入不同的位置信息以示区分。有两种引入位置信息的方式——**位置嵌入（`Position Embeddings`）**和**位置编码（`Position Encodings`）**。其中，位置嵌入与词嵌入类似，即为序列中每个绝对位置赋予一个连续、低维、稠密的向量表示。而位置编码则是使用函数，直接将一个整数（位置索引值）映射到一个d维向量上。映射公式为：

![](4-28.jpg)

式中，<u>p为序列中的位置索引值；0≤i＜d是位置编码向量中的索引值。</u>
无论是使用位置嵌入还是位置编码，在获得一个位置对应的向量后，再与该位置对应的词向量进行相加，即可表示该位置的输入向量。这样即使词向量相同，但是如果它们所处的位置不同，其最终的向量表示也不相同，从而解决了原始自注意力模型无法对序列进行建模的问题。

##### 4.4.3.2 输入向量角色信息

​	原始的自注意力模型在计算注意力时直接使用两个输入向量，然后使用得到的注意力对同一个输入向量加权，这样导致一个输入向量同时承担了三种角色：查询（Query）、键（Key）和值（Value）。更好的做法是，对不同的角色使用不同的向量。为了做到这一点，可以使用不同的参数矩阵对原始的输入向量做线性变换，从而让不同的变换结果承担不同的角色。具体地，分别使用三个不同的参数矩阵**`Wq`、`Wk`**和**`Wv`**将输入向量xi映射为三个新的向量qi=Wqxi、ki=Wkxi和vi=Wvxi，分别表示查询、键和值对应的向量。新的输出向量计算公式为：

![](4-29.jpg)

式中，，其中L为序列的长度。

##### 4.4.3.3 多层注意力

​	原始的自注意力模型仅考虑了序列中任意两个输入序列单元之间的关系，而在实际应用中，往往需要同时考虑更多输入序列单元之间的关系，即更高阶的关系。如果直接建模高阶关系，会导致模型的复杂度过高。一方面，类似于图模型中的消息传播机制（Message Propogation），这种高阶关系可以通过堆叠多层自注意力模型实现。另一方面，类似于多层感知器，如果直接堆叠多层注意力模型，由于每层的变换都是线性的（注意力计算一般使用线性函数），最终模型依然是线性的。因此，为了增强模型的表示能力，**往往在每层自注意力计算之后，增加一个非线性的多层感知器（MLP）模型**。另外，<u>如果将自注意力模型看作特征抽取器，那么多层感知器就是最终的分类器</u>。同时，为了使模型更容易学习，还可以使用**层归一化（`Layer Normalization`）、残差连接（`Residual Connections`）**等深度学习的训练技巧。自注意力层、非线性层以及以上的这些训练技巧，构成了一个更大的Transformer层，也叫作Transformer块（Block），如图4-15所示。

![](4-30.jpg)

​							图4-15 Transformer块4.自注意力计算结果互斥


##### 4.4.3.4 自注意力计算结果互斥

​	由于自注意力结果需要经过归一化，导致即使一个输入和多个其他的输入相关，也无法同时为这些输入赋予较大的注意力值，即自注意力结果之间是互斥的，无法同时关注多个输入。因此，如果能使用多组自注意力模型产生多组不同的注意力结果，则不同组注意力模型可能关注到不同的输入上，从而增强模型的表达能力。那么如何产生多组自注意力模型呢？方法非常简单，<u>只需要设置多组映射矩阵即可，然后将产生的多个输出向量拼接</u>。<u>为了将输出结果作为下一组的输入，还需要将拼接后的输出向量再经过一个线性映射，映射回d维向量</u>。该模型又叫作**多头自注意力（`Multi-head Self-attention`）模型**。从另一方面理解，多头自注意力机制相当于多个不同的自注意力模型的集成（`Ensemble`），也会增强模型的效果。类似卷积神经网络中的多个卷积核，也可以将不同的注意力头理解为抽取不同类型的特征。

#### 4.4.4 基于Transformer的序列到序列模型

​	以上介绍的Transformer模型可以很好地对一个序列编码。此外，与循环神经网络类似，Transformer也可以很容易地实现解码功能，将两者结合起来，就实现了一个序列到序列的模型，于是可以完成机器翻译等多种自然语言处理任务。**解码模块**的实现与编码模块基本相同，<u>不过要接收编码模块的最后一层输出作为输入，这也叫作`记忆（Memory）`，另外还要将已经部分解码的输出结果作为输入</u>，如图4-16所示。

![](4-31.jpg)

​								图4-16 基于Transformer的序列到序列模型示例

#### 4.4.5 Transformer模型的优缺点

优点：	

- 与循环神经网络相比，Transformer能够直接建模输入序列单元之间更长距离的依赖关系，从而使得Transformer<u>对于长序列建模的能力更强。</u>
- 另外，在Trans-former的编码阶段，由于可以利用GPU等多核计算设备并行地计算Transformer块内部的自注意力模型，而循环神经网络需要逐个计算，因此Transformer<u>具有更高的训练速度</u>。

缺点：

- Transformer的一个明显的缺点是参数量过于庞大。每一层的Transformer块大部分参数集中在图4-15中的绿色方框中，即自注意力模型中输入向量的三个角色映射矩阵、多头机制导致相应参数的倍增和引入非线性的多层感知器等。更主要的是，还需要堆叠多层Transformer块，从而参数量又扩大多倍。最终导致一个实用的Transformer模型含有巨大的参数量。以本书后续章节将要介绍的BERT模型为例，BERT-base含有12层Transformer块，参数量超过1.1亿个，而24层的BERT-large，参数量达到了3.4亿个之多。巨大的参数量导致Transformer模型非常不容易训练，尤其是当训练数据较小时。

  缺点产生的结果：

  ​	因此，为了降低模型的训练难度，<u>基于大规模数据的预训练模型应运而生</u>，这也是本书将要介绍的重点内容。唯此，才能发挥Transformer模型强大的表示能力。

#### 4.4.6 模型实现

​	新版本的PyTorch（1.2版及以上）实现了Transformer模型。其中，nn.Trans formerEncoder实现了编码模块，它是由多层Transformer块构成的，每个块使用TransformerEncoderLayer实现。下面演示具体的示例。

![](4-32.jpg)

然后可以将多个Transformer块堆叠起来，构成一个完整的nn.Transformer Encoder。

![](4-33.jpg)

解码模块也类似，TransformerDecoderLayer定义了一个解码模块的Trans-former块，通过多层块堆叠构成nn.TransformerDecoder，下面演示具体的调用方式。

![](4-34.jpg)

![](4-35.jpg)



### 4.5 神经网络模型的训练

​	以上章节介绍了自然语言处理中几种常用的神经网络（深度学习）模型，其中每种模型内部都包含大量的参数，如何恰当地设置这些参数是决定模型准确率的关键，而<u>寻找一组优化参数的过程</u>又叫作**模型训练或学习**。

#### 4.5.1 损失函数

​	为了评估一组参数的好坏，需要有一个准则，在机器学习中，又被称为**损失函数（`Loss Function`）**。简单来讲，损失函数用于<u>衡量在训练数据集上模型的输出与真实输出之间的差异</u>。因此，损失函数的值越小，模型输出与真实输出越相似，可以认为此时模型表现越好。不过如果损失函数的值过小，那么模型就会与训练数据集过拟合（Overfit），反倒不适用于新的数据。所以，在训练深度学习模型时，要避免产生过拟合的现象，有多种技术可以达到此目的，如正则化（`Regularization`）、丢弃正则化（`Dropout`）和早停法（`Early Stopping`）等。
在此介绍深度学习中两种常用的损失函数：

**均方误差（`Mean Squared Error，MSE`）**损失和**交叉熵（`Cross-Entropy，CE`）**损失。所谓均方误差损失指的是每个样本的平均平方损失，即：

![](4-36.jpg)

式中，m表示样本的数目；y（i）表示第i个样本的真实输出结果；表示第i个样本的模型预测结果。可见，模型表现越好，即预测结果与真实结果越相似，均方误差损失越小。
​	以上形式的均方误差损失**适合于回归问题**，即一个样本有一个连续输出值作为标准答案。那么如何使用均方误差损失<u>**处理分类问题**</u>呢？假设处理的是c类分类问题，则均方误差被定义为：

![](4-37.jpg)

式中，y表示第 i个样本的第 j 类上的真实输出结果，只有正确的类别输出为1，其他类别输出为0；y^表示模型对第i个样本的第j类上的预测结果，如果使用Softmax函数对结果进行归一化，则表示对该类别预测的概率。与回归问题的均方误差损失一样，模型表现越好，其对真实类别预测的概率越趋近于1，对于错误类别预测的概率则趋近于0，因此最终计算的损失也越小。
<u>在处理分类问题时</u>，**交叉熵损失**是一种更常用的损失函数。与均方误差损失相比，交叉熵损失的学习速度更快。其具体定义为：

![](4-38.jpg)

式中，表示第 i个样本的第 j 类上的真实输出结果，只有正确的类别输出为1，其他类别输出为0；表示模型对第i个样本属于第j类的预测概率。于是，最终交叉熵损失只取决于模型对正确类别预测概率的对数值。如果模型表现越好，则预测的概率越大，由于公式右侧前面还有一个负号，所以交叉熵损失越小（这符合直觉）。更本质地讲，交叉熵损失函数公式右侧是对多类输出结果的分布（伯努利分布）求极大似然中的对数似然函数（Log-Likelihood）。另外，由于交叉熵损失只取决于正确类别的预测结果，所以其还可以进一步化简，即：

![](4-39.jpg)

​	式中，表示模型对第i个样本在正确类别t上的预测概率。所以，交叉熵损失也被称为**负对数似然损失（Negative Log Likelihood，NLL）**。之所以交叉熵损失的学习速度更高，是因为当模型错误较大时，即对正确类别的预测结果偏小（趋近于0），负对数的值会非常大；而当模型错误较小时，即对正确类别的预测结果偏大（趋近于1），负对数的值会趋近于0。这种变化是呈指数形的，即当模型错误较大时，损失函数的梯度较大，因此模型学得更快；而当模型错误较小时，损失函数的梯度较小，此时模型学得更慢。

#### 4.5.2 梯度下降

**梯度下降（Gradient Descent，GD）**是一种非常基础和常用的参数优化方法。梯度（Gradient）即以向量的形式写出的对多元函数各个参数求得的偏导数。如函数f （x1， x2，···， xn）对各个参数求偏导，则梯度向量为，也可以记为 ∇f （x1， x2，···， xn）。梯度的物理意义是<u>函数值增加最快的方向</u>，或者说，<u>沿着梯度的方向更加容易找到函数的极大值</u>；反过来说，<u>沿着梯度相反的方向，更加容易找到函数的极小值</u>。正是利用了梯度的这一性质，对深度学习模型进行训练时，就可以通过梯度下降法一步步地迭代优化一个事先定义的损失函数，即得到较小的损失函数，并获得对应的模型参数值。梯度下降算法如下所示。

![](4-40.png)

​									算法4.1 梯度下降算法
​	在算法中，循环的终止条件根据实际情况可以有多种，如<u>给定的循环次数、算法两次循环之间梯度变化的差小于一定的阈值和在开发集上算法的准确率不再提升等</u>，读者可以根据实际情况自行设定。
然而，当训练数据的规模比较大时，如果每次都遍历全部的训练数据计算梯度，算法的运行时间会非常久。为了提高算法的运行速度，每次可以随机采样一定规模的训练数据来估计梯度，此时被称为**小批次梯度下降（`Mini-batch Gradient Descent`）**，具体算法如下。
虽然与原始的梯度下降法相比，小批次梯度下降法每次计算的梯度可能不那么准确，但是由于其梯度计算的速度较高，因此可以通过更多的迭代次数弥补梯度计算不准确的问题。当小批次的数目被设为b=1时，则被称为**随机梯度下降（`Stochastic Gradient Descent，SGD`）**。
接下来，以在4.1.6节介绍的多层感知器为例，介绍如何使用梯度下降法获得优化的参数，解决异或问题。代码如下。

![](4-41.png)

​										算法4.2 小批次梯度下降算法

![](4-42.png)

![](4-43.png)

输出结果如下：首先，输出网络的参数值，包括两个线性映射层的权重和偏置项的值；然后，输出网络对训练数据的预测结果，即[0， 1， 1， 0]，其与原训练数据相同，说明该组参数能够正确地处理异或问题（即线性不可分问题）。

![](4-44.png)

需要注意的是，PyTorch提供了`nn.CrossEntropyLoss`损失函数（类），不过与一般意义上的交叉熵损失不同，<u>其在计算损失之前自动进行Softmax计算，因此在网络的输出层不需要再调用Softmax层。</u>这样做的好处是在使用该模型预测时可以提高速度，因为没有进行Softmax运算，直接将输出分数最高的类别作为预测结果即可。除了nn.NLLLoss和nn.CrossEntropyLoss，PyTorch还定义了很多其他常用的损失函数，本书不再进行介绍，感兴趣的读者请参考PyTorch的官方文档。
​	同样地，除了梯度下降，PyTorch还提供了其他的优化器，如Adam、Adagrad和Adadelta等，这些优化器是对原始梯度下降法的改进，改进思路包括动态调整学习率、对梯度累积等。它们的调用方式也非常简单，只要在定义优化器时替换为相应的优化器类，并提供一些必要的参数即可。关于这些优化器的定义、区别和联系，本书也不再介绍，感兴趣的读者请参考其他深度学习类书籍。

### 4.6 情感分类实战

​	本节以句子情感极性分类为例，演示如何使用PyTorch实现上面介绍的四种深度学习模型，即<u>多层感知器、卷积神经网络、LSTM和Transformer</u>，来解决文本分类问题。为了完成此项任务，还需要编写<u>词表映射、词向量层、融入词向量层的多层感知器数据处理、文本表示和模型的训练与测试</u>等辅助功能，下面分别加以介绍。

#### 4.6.1 词表映射

​	无论是使用深度学习，还是传统的统计机器学习方法处理自然语言，首先都需要将输入的语言符号，通常为**标记**（`Token`），<u>映射为大于等于0、小于词表大小的整数，该整数也被称作一个标记的索引值或下标</u>。本书编写了一个Vocab（**词表，`Vocabulary`**）类实现标记和索引之间的相互映射。完整的代码如下。

```python
from collections import defaultdict, Counter

class Vocab:
    def __init__(self, tokens=None):
        self.idx_to_token = list()
        self.token_to_idx = dict()

        if tokens is not None:
            if "<unk>" not in tokens:
                tokens = tokens + ["<unk>"]
            for token in tokens:
                self.idx_to_token.append(token)
                self.token_to_idx[token] = len(self.idx_to_token) - 1
            self.unk = self.token_to_idx['<unk>']

    @classmethod
    def build(cls, text, min_freq=1, reserved_tokens=None):
        token_freqs = defaultdict(int)
        for sentence in text:
            for token in sentence:
                token_freqs[token] += 1
        uniq_tokens = ["<unk>"] + (reserved_tokens if reserved_tokens else [])
        uniq_tokens += [token for token, freq in token_freqs.items() \
                        if freq >= min_freq and token != "<unk>"]
        return cls(uniq_tokens)

    def __len__(self):
        return len(self.idx_to_token)

    def __getitem__(self, token):
        return self.token_to_idx.get(token, self.unk)

    def convert_tokens_to_ids(self, tokens):
        return [self[token] for token in tokens]

    def convert_ids_to_tokens(self, indices):
        return [self.idx_to_token[index] for index in indices]


def save_vocab(vocab, path):
    with open(path, 'w') as writer:
        writer.write("\n".join(vocab.idx_to_token))


def read_vocab(path):
    with open(path, 'r') as f:
        tokens = f.read().split('\n')
    return Vocab(tokens)


```



![](4-45.png)

![](4-46.png)

#### 4.6.2 词向量层

​	如在本书文本表示部分（2.1节）介绍的，在使用深度学习进行自然语言处理时，<u>将一个词（或者标记）转换为一个低维、稠密、连续的词向量（也称Embed-ding）是一种基本的词表示方法</u>，通过torch.nn包提供的Embedding层即可实现该功能。创建Embedding对象时，需要提供两个参数，分别是num_embeddings，即词表的大小；以及embedding_dim，即Embedding向量的维度。调用该对象实现的功能是将输入的整数张量中每个整数（通过词表映射功能获得标记对应的整数）映射为相应维度（embedding_dim）的张量。如下面的例子所示。

![](4-47.png)

![](4-48.jpg)

#### 4.6.3 融入词向量层的多层感知器

​	在4.1.6节中介绍了基本的多层感知器实现方式，其输入为固定大小的实数向量。如果输入为文本，即整数序列（假设已经利用词表映射工具将文本中每个标记映射为了相应的整数），在经过多层感知器之前，需要利用词向量层将输入的整数映射为向量。
​	但是，一个序列中通常含有多个词向量，那么如何将它们表示为一个多层感知器的输入向量呢？一种方法是将n个向量拼接成一个大小为n×d的向量，其中d表示每个词向量的大小。不过，这样做的一个问题是最终的预测结果与标记在序列中的位置过于相关。例如，如果在一个序列前面增加一个标记，则序列中的每个标记位置都变了，也就是它们对应的参数都发生了变化，那么模型预测的结果可能完全不同，这样显然不合理。在自然语言处理中，可以使用**词袋（`Bag-Of-Words，BOW`）**模型解决该问题。词袋模型<u>指的是在表示序列时，不考虑其中元素的顺序，而是将其简单地看成是一个集合</u>。于是就可以采用聚合操作处理一个序列中的多个词向量，如求平均、求和或保留最大值等。融入词向量层以及词袋模型的多层感知器代码如下：

```python
import torch
from torch import nn
from torch.nn import functional as F

class MLP(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_class):
        super(MLP, self).__init__()
        # 词嵌入层
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        # 线性变换：词嵌入层->隐含层
        self.linear1 = nn.Linear(embedding_dim, hidden_dim)
        # 使用ReLU激活函数
        self.activate = F.relu
        # 线性变换：激活层->输出层
        self.linear2 = nn.Linear(hidden_dim, num_class)

    def forward(self, inputs):
        embeddings = self.embedding(inputs)
        # 将序列中多个embedding进行聚合（此处是求平均值）
        embedding = embeddings.mean(dim=1)
        hidden = self.activate(self.linear1(embedding))
        outputs = self.linear2(hidden)
        # 获得每个序列属于某一类别概率的对数值
        probs = F.log_softmax(outputs, dim=1)
        return probs

mlp = MLP(vocab_size=8, embedding_dim=3, hidden_dim=5, num_class=2)
# 输入为两个长度为4的整数序列
inputs = torch.tensor([[0, 1, 2, 1], [4, 6, 6, 7]], dtype=torch.long)
outputs = mlp(inputs)
print(outputs)
```

![](4-49.png)

![](4-50.png)

最终的输出结果为每个序列属于某一类别概率的对数值。

![](4-51.jpg)

![](4-52.jpg)

然而，在实际的自然语言处理任务中，一个批次里输入的文本长度往往是不固定的，因此无法像上面的代码一样简单地用一个张量存储词向量并求平均值。PyTorch提供了一种更灵活的解决方案，即`EmbeddingBag`层。在调用Embedding-Bag层时，首先需要将不定长的序列拼接起来，然后使用一个偏移向量（Offsets）记录每个序列的起始位置。举个例子，假设一个批次中有4个序列，长度分别为4、5、3和6，将这些长度值构成一个列表，并在前面加入0（第一个序列的偏移量），构成列表offsets=[0，4，5，3，6]，然后使用语句torch.tensor（offsets [：-1]）获得张量[0，4，5，3]，后面紧接着执行cumsum（dim=0）方法（累加），获得新的张量[0，4，9，12]，这就是最终每个序列起始位置的偏移向量。下面展示相应的代码示例。

![](4-53.png)

使用词袋模型表示文本的一个<u>天然缺陷是没有考虑词的顺序</u>。为了更好地对文本序列进行表示，还可以将词的N-gram（n元组）当作一个标记，这样相当于考虑了词的局部顺序信息，不过同时也增加了数据的稀疏性，<u>因此 n 不宜过大（一般为2或3）</u>。在此，将N-gram作为标记的实现方法留作思考题，请读者自行实现。

#### 4.6.4 数据处理

​	数据处理的第一步自然是将待处理的数据从硬盘或者其他地方加载到程序中，此时读入的是原始文本数据，还需要经过第3章介绍的分句、标记解析等预处理过程转换为标记序列，然后再使用词表映射工具将每个标记映射到相应的索引值。在此，使用 NLTK 提供的句子倾向性分析数据（sentence_polarity）作为示例，具体代码如下。

![](4-54.png)

通过以上函数加载的数据不太方便直接给PyTorch使用，因此PyTorch提供了DataLoader类（在torch.utils.data包中）。通过创建和调用该类的对象，可以在训练和测试模型时方便地实现数据的采样、转换和处理等功能。例如，使用下列语句创建一个DataLoader对象。

![](4-55.jpg)

![](4-56.jpg)

以上代码提供了四个参数，其中batch_size和shuffle较易理解，分别为每一步使用的小批次（Mini-batch）的大小以及是否对数据进行随机采样；而参数dataset和collate_fn则不是很直观，下面分别进行详细的介绍。

dataset是Dataset类（在torch.utils.data包中定义）的一个对象，用于存储数据，一般需要根据具体的数据存取需求创建Dataset类的子类。如创建一个BowDataset子类，其中Bow是词袋的意思。具体代码如下。

![](4-57.png)

collate_fn参数指向一个函数，用于对一个批次的样本进行整理，如将其转换为张量等。具体代码如下。

![](4-58.png)

#### 4.6.5 多层感知器模型的训练与测试

对创建的多层感知器模型，就是用实际的数据进行训练与测试。

![](4-59.png)

![](4-60.png)

#### 4.6.6 基于卷积神经网络的情感分类

​	当使用4.6.3节介绍的词袋模型表示文本时，只考虑了文本中词语的信息，而忽视了词组信息，如句子“我不喜欢这部电影”，词袋模型看到文本中有“喜欢”一词，则很可能将其识别为褒义。而卷积神经网络可以提取词组信息，如将卷积核的大小设置为2，则可以提取特征“不喜欢”等，显然这对于最终情感极性的判断至关重要。卷积神经网络的大部分代码与多层感知器的实现一致，下面仅对其中的不同之处加以说明。
首先是模型不同，需要从nn.Module类派生一个CNN子类。

![](4-61.png)

![](4-62.jpg)

在调用卷积神经网络时，还需要设置两个额外的超参数，分别为filter_size=3（卷积核的大小）和num_filter=100（卷积核的个数）。
另外，**数据整理函数**`collate_fn`也需要进行一些修改。

![](4-63.jpg)

​	在代码中，pad_sequence函数实现补齐（Padding）功能，使得一个批次中全部序列长度相同（同最大长度序列），不足的默认使用0补齐。
​	除了以上两处不同，其他代码与多层感知器的实现几乎一致。由此可见，如要实现一个基于新模型的情感分类任务，只需要定义一个nn.Module类的子类，并修改数据整理函数（collate_fn）即可，这也是使用PyTorch等深度学习框架的优势。

具体代码：

```python
# Defined in Section 4.6.6

import torch
from torch import nn, optim
from torch.nn import functional as F
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from collections import defaultdict
from vocab import Vocab
from utils import load_sentence_polarity

import nltk
nltk.download('sentence_polarity')

class CnnDataset(Dataset):
    def __init__(self, data):
        self.data = data
    def __len__(self):
        return len(self.data)
    def __getitem__(self, i):
        return self.data[i]

def collate_fn(examples):
    inputs = [torch.tensor(ex[0]) for ex in examples]
    targets = torch.tensor([ex[1] for ex in examples], dtype=torch.long)
    # 对batch内的样本进行padding，使其具有相同长度
    inputs = pad_sequence(inputs, batch_first=True)
    return inputs, targets

class CNN(nn.Module):
    def __init__(self, vocab_size, embedding_dim, filter_size, num_filter, num_class):
        super(CNN, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.conv1d = nn.Conv1d(embedding_dim, num_filter, filter_size, padding=1)
        self.activate = F.relu
        self.linear = nn.Linear(num_filter, num_class)
    def forward(self, inputs):
        embedding = self.embedding(inputs)
        convolution = self.activate(self.conv1d(embedding.permute(0, 2, 1)))
        pooling = F.max_pool1d(convolution, kernel_size=convolution.shape[2])
        outputs = self.linear(pooling.squeeze(dim=2))
        log_probs = F.log_softmax(outputs, dim=1)
        return log_probs

#tqdm是一个Pyth模块，能以进度条的方式显示迭代的进度
from tqdm.auto import tqdm

#超参数设置
embedding_dim = 128
hidden_dim = 256
num_class = 2
batch_size = 32
num_epoch = 5
filter_size = 3
num_filter = 100

#加载数据
train_data, test_data, vocab = load_sentence_polarity()
train_dataset = CnnDataset(train_data)
test_dataset = CnnDataset(test_data)
train_data_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)
test_data_loader = DataLoader(test_dataset, batch_size=1, collate_fn=collate_fn, shuffle=False)

#加载模型
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = CNN(len(vocab), embedding_dim, filter_size, num_filter, num_class)
model.to(device) #将模型加载到CPU或GPU设备

#训练过程
nll_loss = nn.NLLLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001) #使用Adam优化器

model.train()
for epoch in range(num_epoch):
    total_loss = 0
    for batch in tqdm(train_data_loader, desc=f"Training Epoch {epoch}"):
        inputs, targets = [x.to(device) for x in batch]
        log_probs = model(inputs)
        loss = nll_loss(log_probs, targets)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Loss: {total_loss:.2f}")

#测试过程
acc = 0
for batch in tqdm(test_data_loader, desc=f"Testing"):
    inputs, targets = [x.to(device) for x in batch]
    with torch.no_grad():
        output = model(inputs)
        acc += (output.argmax(dim=1) == targets).sum().item()

#输出在测试集上的准确率
print(f"Acc: {acc / len(test_data_loader):.2f}")

```

#### 4.6.7 基于循环神经网络的情感分类

​	4.6.3 节介绍的词袋模型还忽略了文本中词的顺序信息，因此对于两个句子“张三打李四”和“李四打张三”，它们的表示是完全相同的，但显然这并不合理。循环神经网络模型能更好地对序列数据进行表示。本节以长短时记忆（LSTM）网络为例，介绍如何使用循环神经网络模型解决情感分类问题。其中，大部分代码与前面的实现一致，下面仅对其中的不同之处加以说明。
首先，需要从nn.Module类派生一个LSTM子类。

![](4-64.jpg)

​	代码中，大部分内容在前面的章节都已介绍过，只有pack_padded_sequence函数需要特别说明。其实现的功能是<u>将之前经过补齐的一个小批次序列打包成一个序列，其中每个原始序列的长度存储在lengths中。该打包序列能够被self.lstm对象直接调用。</u>
另一个主要不同是数据整理函数，具体代码如下。

![](4-65.png)

在代码中，lengths用于存储每个序列的长度。除此之外，其他代码与多层感知器或卷积神经网络的实现几乎一致。

完整代码：

```python
# Defined in Section 4.6.7

import torch
from torch import nn, optim
from torch.nn import functional as F
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence
from collections import defaultdict
from vocab import Vocab
from utils import load_sentence_polarity

#tqdm是一个Python模块，能以进度条的方式显式迭代的进度
from tqdm.auto import tqdm

class LstmDataset(Dataset):
    def __init__(self, data):
        self.data = data
    def __len__(self):
        return len(self.data)
    def __getitem__(self, i):
        return self.data[i]

def collate_fn(examples):
    lengths = torch.tensor([len(ex[0]) for ex in examples])
    inputs = [torch.tensor(ex[0]) for ex in examples]
    targets = torch.tensor([ex[1] for ex in examples], dtype=torch.long)
    # 对batch内的样本进行padding，使其具有相同长度
    inputs = pad_sequence(inputs, batch_first=True)
    return inputs, lengths, targets

class LSTM(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_class):
        super(LSTM, self).__init__()
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.output = nn.Linear(hidden_dim, num_class)

    def forward(self, inputs, lengths):
        embeddings = self.embeddings(inputs)
        x_pack = pack_padded_sequence(embeddings, lengths, batch_first=True, enforce_sorted=False)
        hidden, (hn, cn) = self.lstm(x_pack)
        outputs = self.output(hn[-1])
        log_probs = F.log_softmax(outputs, dim=-1)
        return log_probs

embedding_dim = 128
hidden_dim = 256
num_class = 2
batch_size = 32
num_epoch = 5

#加载数据
train_data, test_data, vocab = load_sentence_polarity()
train_dataset = LstmDataset(train_data)
test_dataset = LstmDataset(test_data)
train_data_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)
test_data_loader = DataLoader(test_dataset, batch_size=1, collate_fn=collate_fn, shuffle=False)

#加载模型
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = LSTM(len(vocab), embedding_dim, hidden_dim, num_class)
model.to(device) #将模型加载到GPU中（如果已经正确安装）

#训练过程
nll_loss = nn.NLLLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001) #使用Adam优化器

model.train()
for epoch in range(num_epoch):
    total_loss = 0
    for batch in tqdm(train_data_loader, desc=f"Training Epoch {epoch}"):
        inputs, lengths, targets = [x.to(device) for x in batch]
        log_probs = model(inputs, lengths)
        loss = nll_loss(log_probs, targets)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Loss: {total_loss:.2f}")

#测试过程
acc = 0
for batch in tqdm(test_data_loader, desc=f"Testing"):
    inputs, lengths, targets = [x.to(device) for x in batch]
    with torch.no_grad():
        output = model(inputs, lengths)
        acc += (output.argmax(dim=1) == targets).sum().item()

#输出在测试集上的准确率
print(f"Acc: {acc / len(test_data_loader):.2f}")

```

#### 4.6.8 基于Transformer的情感分类

基于Transformer实现情感分类与使用LSTM也非常相似，主要有一处不同，即需要定义Transformer模型。具体代码如下。

![](4-67.jpg)

![](4-68.png)

在代码中，length_to_mask函数比较关键，其作用是<u>根据批次中每个序列长度生成Mask矩阵，以便处理长度不一致的序列，忽略掉比较短的序列的无效部分。</u>同时，也是TransformerEncoder中调用函数所需的src_key_padding_mask参数。具体代码如下。

![](4-69.jpg)

![](4-70.jpg)

​	不过，由于src_key_padding_mask参数正好与length_to_mask函数生成的结果相反（无自注意力部分为True），因此还需要取反，即length_to_mask（lengths）==False。
​	另外，此处使用了位置编码（Position Encodings），所以还需要自行实现。当然也可以使用位置嵌入（Position Embeddings），这样只需调用 PyTorch 提供的nn.Embedding层即可。位置编码层的实现方式如下。

![](4-71.png)

### 4.7 词性标注实战

本节介绍如何使用前面介绍的深度学习模型，实现一个词性标注系统，该系统也可以扩展实现其他的序列标注任务。

#### 4.7.1 基于前馈神经网络的词性标注

​	首先介绍如何使用多层感知器实现词性标注。与情感分类类似，可以将词性标注任务看作多类别文本分类问题，即<u>取目标词的上下文词作为输入，目标词的词性作为输出类别</u>。由于上下文一般不取太大（如除目标词自身外，还可以左右各取一或两个词），而且上下文中的词所处位置对于目标词的词性判断也比较关键（如一个词在目标词的左侧还是右侧的意义并不相同），因此一般将上下文的词向量进行拼接，构成多层感知器的输入。这种方法又叫作基于窗口（Window）的方法。
​	与多层感知器类似，可以用另外一种前馈神经网络，即卷积神经网络实现词性标注。与多层感知器不同的是，可以使用卷积神经网络对更长的上下文进行表示。
​	从代码角度来讲，两种前馈神经网络实现的大部分代码与文本分类问题（如4.6节介绍的情感分类问题）的实现是相同的，只是数据处理稍有不同，因此在此不再赘述，读者可自行实现。

#### 4.7.2 基于循环神经网络的词性标注

​	基于多层感知器的词性标注每次只能取有限的上下文作为模型的输入，而基于循环神经网络的模型可以使用更长的上下文，因此更适合序列标注问题。此处以NLTK提供的宾州树库（Penn Treebank）样例数据为例，介绍如何使用LSTM循环神经网络进行词性标注。
​	首先加载宾州树库的词性标注语料库，代码如下。

![](4-72.png)

![](4-73.jpg)

然后，可以通过执行num_class=len（pos_vocab）获得类别数，即词性标签的个数。接下来还需要修改collate_fn函数。

![](4-74.png)

模型部分基本与文本分类中的一致，除了以下代码中注释标注的两行。

![](4-75.png)

![](4-76.jpg)

最后，在训练阶段和预测阶段，需要使用mask来保证仅对有效的标记求损失、对正确预测结果以及总的标记计数。即`loss=nll_loss（log_probs[mask]，targets[mask]），acc+=（output.argmax（dim=-1）==targets）[mask].sum（）.item（）和total+=mask.sum（）.item（）`。

#### 4.7.3 基于Transformer的词性标注

​	基于Transformer实现词性标注相当于将基于Transformer实现的情感分类与基于 LSTM 实现的词性标注相融合。其中，collate_fn 函数与 LSTM 词性标注中的相同。Transformer 层的实现与 Transformer 情感分类基本相同，只有在forward函数中需要取序列中每个输入对应的隐含层并计算概率，而不是第1个输入的隐含层（代表整个序列）。具体修改如下。

![](4-74.png)

```python
# Defined in Section 4.7.3

import math
import torch
from torch import nn, optim
from torch.nn import functional as F
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence
from collections import defaultdict
from vocab import Vocab
from utils import load_treebank, length_to_mask

#tqdm是一个Pyth模块，能以进度条的方式显式迭代的进度
from tqdm.auto import tqdm

class TransformerDataset(Dataset):
    def __init__(self, data):
        self.data = data
    def __len__(self):
        return len(self.data)
    def __getitem__(self, i):
        return self.data[i]

def collate_fn(examples):
    lengths = torch.tensor([len(ex[0]) for ex in examples])
    inputs = [torch.tensor(ex[0]) for ex in examples]
    targets = [torch.tensor(ex[1]) for ex in examples]
    # 对batch内的样本进行padding，使其具有相同长度
    inputs = pad_sequence(inputs, batch_first=True, padding_value=vocab["<pad>"])
    targets = pad_sequence(targets, batch_first=True, padding_value=vocab["<pad>"])
    return inputs, lengths, targets, inputs != vocab["<pad>"]

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=512):
        super(PositionalEncoding, self).__init__()

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return x

class Transformer(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_class,
                 dim_feedforward=512, num_head=2, num_layers=2, dropout=0.1, max_len=512, activation: str = "relu"):
        super(Transformer, self).__init__()
        # 词嵌入层
        self.embedding_dim = embedding_dim
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.position_embedding = PositionalEncoding(embedding_dim, dropout, max_len)
        # 编码层：使用Transformer
        encoder_layer = nn.TransformerEncoderLayer(hidden_dim, num_head, dim_feedforward, dropout, activation)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
        # 输出层
        self.output = nn.Linear(hidden_dim, num_class)

    def forward(self, inputs, lengths):
        inputs = torch.transpose(inputs, 0, 1)
        hidden_states = self.embeddings(inputs)
        hidden_states = self.position_embedding(hidden_states)
        attention_mask = length_to_mask(lengths) == False
        hidden_states = self.transformer(hidden_states, src_key_padding_mask=attention_mask).transpose(0, 1)
        logits = self.output(hidden_states)
        log_probs = F.log_softmax(logits, dim=-1)
        return log_probs

embedding_dim = 128
hidden_dim = 128
batch_size = 32
num_epoch = 5

#加载数据
train_data, test_data, vocab, pos_vocab = load_treebank()
train_dataset = TransformerDataset(train_data)
test_dataset = TransformerDataset(test_data)
train_data_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)
test_data_loader = DataLoader(test_dataset, batch_size=1, collate_fn=collate_fn, shuffle=False)

num_class = len(pos_vocab)

#加载模型
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = Transformer(len(vocab), embedding_dim, hidden_dim, num_class)
model.to(device) #将模型加载到GPU中（如果已经正确安装）

#训练过程
nll_loss = nn.NLLLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001) #使用Adam优化器

model.train()
for epoch in range(num_epoch):
    total_loss = 0
    for batch in tqdm(train_data_loader, desc=f"Training Epoch {epoch}"):
        inputs, lengths, targets, mask = [x.to(device) for x in batch]
        log_probs = model(inputs, lengths)
        loss = nll_loss(log_probs[mask], targets[mask])
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Loss: {total_loss:.2f}")

#测试过程
acc = 0
total = 0
for batch in tqdm(test_data_loader, desc=f"Testing"):
    inputs, lengths, targets, mask = [x.to(device) for x in batch]
    with torch.no_grad():
        output = model(inputs, lengths)
        acc += (output.argmax(dim=-1) == targets)[mask].sum().item()
        total += mask.sum().item()

#输出在测试集上的准确率
print(f"Acc: {acc / total:.2f}")

```

## 第五章 静态词向量预训练模型

​	文本的有序性以及词与词之间的共现信息为自然语言处理提供了天然的自监督学习信号，使得系统无须额外人工标注也能够从文本中习得知识。本章将介绍几种静态词向量的预训练技术，主要包括基于**语言模型**和基于**词共现**两类方法，展示如何从未标注文本中通过自监督学习获取单词级别的语义表示，并提供常用模型的具体代码实现。

### 5.1 神经网络语言模型

#### 5.1.1 概述

​	本书2.2.1节介绍了语言模型的基本概念，以及经典的基于离散符号表示的N元语言模型（`N-gram Language Model`）。从语言模型的角度来看，N元语言模型存在明显的缺点。首先，模型容易受到数据稀疏的影响，一般需要对模型进行平滑处理；其次，无法对长度超过N的上下文依赖关系进行建模。神经网络语言模型（`Neural Network Language Model`）在一定程度上克服了这些问题。一方面，通过引入词的分布式表示，也就是词向量（2.1.3节），大大缓解了数据稀疏带来的影响；另一方面，利用更先进的神经网络模型结构（如循环神经网络、Transformer等），可以对长距离上下文依赖进行有效的建模。
​	正因为这些优异的特性，加上语言模型任务本身无须人工标注数据的优势，神经网络语言模型几乎已经替代N元语言模型，成为现代自然语言处理中最重要的基础技术之一；同时，也是本书重点关注的自然语言预训练技术的核心。本节将从最基本的前馈神经网络语言模型出发，介绍如何在大规模无标注文本数据上进行静态词向量的预训练；然后，介绍基于循环神经网络的语言模型，通过引入更丰富的长距离历史信息，进一步提升静态词向量的表示能力。

#### 5.1.2 预训练任务

​	给定一段文本 w1w2··· wn，语言模型的基本任务是根据历史上下文对下一时刻的词进行预测，也就是计算条件概率P （wt|w1w2··· wt−1）。为了构建语言模型，可以将其转化为以词表为类别标签集合的分类问题，其输入为历史词序列w1w2··· wt−1（也记作w1：t−1），输出为目标词wt。然后就可以从无标注的文本语料中构建训练数据集，并通过优化该数据集上的分类损失（如交叉熵损失或负对数似然损失，见4.5节）对模型进行训练。由于监督信号来自数据自身，因此这种学习方式也被称为**自监督学习**（`Self-supervised Learning`）。
​	在讨论模型的具体实现方式之前，首先面临的一个问题是：如何处理动态长度的历史词序列（模型输入）？一个直观的想法是使用词袋表示，但是这种表示方式忽略了词的顺序信息，语义表达能力非常有限。本节将介绍前馈神经网络语言模型（`Feed-forward Neural Network Language Model`）以及循环神经网络语言模型（`Recurrent Neural Network Language Model，RNNLM`），分别从数据和模型的角度解决这一问题。

##### 5.1.2.1 前馈神经网络语言模型

​	前馈神经网络语言模型[4]利用了传统N元语言模型中的马尔可夫假设（Markov Assumption）——对下一个词的预测只与历史中最近的 n−1个词相关。从形式上看：

![](5-1.jpg)

​	因此，模型的输入变成了长度为n−1的定长词序列wt−n+1：t−1，模型的任务也转化为对条件概率P （wt|wt−n+1：t−1）进行估计。
​	前馈神经网络由输入层、词向量层、隐含层和输出层构成。在前馈神经网络语言模型中，词向量层首先对输入层长为n−1的历史词序列wt−n+1：t−1进行编码，将每个词表示为一个低维的实数向量，即词向量；然后，隐含层对词向量层进行线性变换，并使用激活函数实现非线性映射；最后，输出层通过线性变换将隐含层向量映射至词表空间，再通过Softmax函数得到在词表上的归一化的概率分布，如图5-1所示。

![](5-0.jpg)

​							图5-1 前馈神经网络语言模型示意图

（1）输入层。模型的输入层由当前时刻t的历史词序列wt−n+1：t−1构成，主要为离散的符号表示。在具体实现中，既可以使用每个词的**独热编码**（`One-Hot Encoding`），也可以直接使用每个词在词表中的位置下标。

（2）词向量层。词向量层将输入层中的每个词分别映射至一个低维、稠密的实值特征向量。词向量层也可以理解为一个**查找表**（`Look-up Table`），获取词向量的过程，也就是根据词的索引从查找表中找出对应位置的向量的过程。

![](5-2.jpg)

​	式中，表示词w的d维词向量（，为词表）；表示历史序列中所有词向量拼接之后的结果。若定义词向量矩阵为，那么vw即为E中与w对应的列向量，也可以表示为E与w的独热编码ew之间的点积。

（3）隐含层。模型的隐含层对词向量层x进行线性变换与激活。令Whid∈为输入层到隐含层之间的线性变换矩阵，为偏置项，m为隐含层维度。隐含层可以表示为：

![](5-3.jpg)

式中，f是激活函数。常用的激活函数有Sigmoid、tanh和ReLU等，参考第4章的介绍。

（4）输出层。模型的输出层对h做线性变换，并利用Softmax函数进行归一化，从而获得词表V空间内的概率分布。令为隐含层到输出层之间的线性变换矩阵，相应的偏置项为bout。输出层可由下式计算：

![](5-4.jpg)

​	综上所述，前馈神经网络语言模型的自由参数包含<u>词向量矩阵E</u>，词向量层与隐含层之间的<u>权值矩阵 Whid</u>及<u>偏置项 bhid</u>，隐含层与输出层之间的<u>权值矩阵Wout</u>与<u>偏置项bout，</u>可以记为：`θ={E, Whid, bhid, Wout, bout}`
参数数量为，即+m（1+（n−1）d）。由于m和d是常数，所以，模型的自由参数数量随词表大小呈线性增长，且n的增大并不会显著增加参数的数量。另外，词向量维度d、隐含层维度m和输入序列长度n−1等超参数的调优需要在开发集上进行。
​	**模型训练完成后，矩阵E则为预训练得到的静态词向量。**

##### 5.1.2.2 循环神经网络语言模型

​	在前馈神经网络语言模型中，对下一个词的预测需要回看多长的历史是由超参数n决定的。但是，不同的句子对历史长度n的期望往往是变化的。例如，对于句子“他 喜欢 吃 苹果”，根据“吃”容易推测出，下画线处的词有很大概率是一种食物。因此，只需要考虑较短的历史就足够了。而对于结构较为复杂的句子，如“他 感冒 了，于是 下班 之后 去 了 医院”，则需要看到较长的历史（“感冒”）才能合理地预测出目标词“医院”。
​	循环神经网络语言模型[5]正是为了处理这种不定长依赖而设计的一种语言模型。循环神经网络是用来处理序列数据的一种神经网络（4.3节），而自然语言正好满足这种序列结构性质。循环神经网络语言模型中的每一时刻都维护一个隐含状态，该状态蕴含了当前词的所有历史信息，且与当前词一起被作为下一时刻的输入。这个随时刻变化而不断更新的隐含状态也被称作**记忆（`Memory`）**。
​	图5-2展示了循环神经网络语言模型的基本结构。

![](5-5.jpg)

​								图5-2 循环神经网络语言模型的基本结构

（1）输入层。与前馈神经网络语言模型不同，由于模型不再受限于历史上下文的长度，所以此时输入层可由完整的历史词序列构成，即w1：t−1。

（2）词向量层。与前馈神经网络语言模型类似，输入的词序列首先由词向量层映射至相应的词向量表示。那么，在t时刻的输入将由其前一个词wt−1的词向量以及t−1时刻的隐含状态ht−1组成。令w0为句子起始标记（如“＜bos＞”）， h0为初始隐含层向量（可使用0向量），则t时刻的输入可以表示为：

![](5-6.jpg)

（3）隐含层。隐含层的计算与前馈神经网络语言模型类似，由线性变换与激活函数构成。
​	式中。Whid实际上由两部分构成，即Whid=[U；V]，、分别是、ht−1与隐含层之间的权值矩阵。为了体现循环神经网络的递归特性，在书写时常常将两者区分开：

![](5-7.jpg)

（4）输出层。最后，在输出层计算t时刻词表上的概率分布：

![](5-8.jpg)

式中，。
​	以上只是循环神经网络最基本的形式，当序列较长时，训练阶段会存在**梯度弥散**（`Vanishing gradient`）或者**梯度爆炸**（`Exploding gradient`）的风险。为了应对这一问题，以前的做法是在梯度反向传播的过程中按长度进行截断（Truncated Back-propagation Through Time），从而使得模型能够得到有效的训练，但是与此同时，也减弱了模型对于长距离依赖的建模能力。这种做法一直持续到2015年左右，之后被含有门控机制的循环神经网络，如长短时记忆网络（LSTM）（4.3节）代替。

#### 5.1.3 模型实现

##### 5.1.3.1 数据准备

​	本章将使用NLTK中提供的Reuters语料库，该语料库被广泛用于文本分类任务，其中包含10788篇新闻类文档，每篇文档具有1个或多个类别。这里忽略数据中的文本类别信息，而只使用其中的文本数据进行词向量的训练。由于在语言模型的训练过程中需要引入一些预留的标记，例如句首标记、句尾标记，以及在构建批次（Batch）时用于补齐序列长度的标记（Padding token）等，因此首先定义以下常量：

![](5-9.jpg)

​	然后，加载Reuters语料库并构建数据集，同时建立词表，这里需要用到第4章的Vocab类。

![](5-10.png)

​	接下来，将分别给出前馈神经网络语言模型与循环神经网络语言模型的Py-Torch 实现。本章所有模型的实现都将按照“数据+模型+训练算法”的框架组织。

##### 5.1.3.2 前馈神经网络语言模型

（1）数据。首先，创建前馈神经网络语言模型的数据处理类NGramDataset。该类将实现前馈神经网络语言模型的训练数据构建与存取功能。具体代码如下。

![](5-11.png)

（2）模型。接下来，创建前馈神经网络语言模型类FeedForwardNNLM，模型的参数主要包含词向量层、由词向量层到隐含层，由隐含层再到输出层的线性变换参数。具体代码如下。

![](5-12.png)

（3）训练。在数据与模型都构建完成后，可以对模型进行训练，并在训练完成后导出词向量矩阵。具体实现如下。

![](5-13.png)

![](5-14.png)

其中，save_pretrained函数用于保存词表以及训练得到的词向量。

![](5-15.png)

​	将每轮迭代的模型损失绘制成曲线，如图5-3所示。可以看到，模型在训练集上的损失随着迭代轮次的增加而不断减小。需要注意的是，由于训练的目标是获取词向量而不是语言模型本身，所以在以上训练过程中，并不需要以模型达到收敛状态（损失停止下降）作为训练终止条件。在实际应用中，由于通常训练数据规模较大，在整个数据集上迭代一定次数之后，便可以获得质量较好的词向量。

![](5-16.jpg)

​								图5-3 训练过程中模型损失的变化曲线

##### 5.1.3.3 循环神经网络语言模型

（1）数据。第一步仍然是创建循环神经网络语言模型的数据类RnnlmDataset，实现训练数据的构建与存取。这里使用序列预测的方式构建训练样本。具体的，对于句子w1w2··· wn，循环神经网络的输入序列为＜bos＞w1w2··· wn，输出序列为w1w2··· wn＜eos＞。与基于定长上下文的前馈神经网络语言模型不同，<u>RNNLM的输入序列长度是动态变化的，因此在构建批次时，需要对批次内样本进行补齐，使其长度一致</u>。这里使用PyTorch库的**`pad_sequence`**函数对不定长的序列进行自动补全并构建样本批次，具体代码如下。

![](5-17.png)

![](5-18.png)

（2）模型。创建循环神经网络语言模型类RNNLM。循环神经网络语言模型主要包含<u>词向量层</u>、<u>循环神经网络</u>（这里使用LSTM）和<u>输出层</u>。具体代码如下。

![](5-19.png)

（3）训练。模型的训练过程与前馈神经网络语言模型的训练基本一致。由于输入输出序列可能较长，因此可以视情况调整批次大小（batch_size）。

![](5-20.jpg)

可以设置**`ignore_index`**参数，以忽略**`PAD_TOKEN`**出的损失。

![](5-21.png)

### 5.2 Word2vec词向量

#### 5.2.1 概述

​	从词向量学习的角度来看，基于神经网络语言模型的预训练方法存在一个明显的缺点，即当对t时刻词进行预测时，模型只利用了历史词序列作为输入，而损失了与“未来”上下文之间的共现信息。本节将介绍一类训练效率更高、表达能力更强的**词向量预训练模型**——Word2vec [6]，其中包括**`CBOW`**（**`Continuous Bag-of-Words`**）模型以及**`Skip-gram`**模型。这两个模型由Tomas Mikolov等人于2013年提出，它们不再是严格意义上的语言模型，<u>完全基于词与词之间的共现信息实现词向量的学习</u>。相应的开源工具word2vec被自然语言处理学术界和工业界广泛使用。

##### 5.2.1.1 CBOW模型

​	给定一段文本，CBOW模型的基本思想是根据上下文对目标词进行预测。例如，对于文本······，CBOW模型的任务是根据一定窗口大小内的上下文Ct（若取窗口大小为5，则Ct={wt−2， wt−1， wt+1， wt+2}）对t时刻的词wt进行预测。与神经网络语言模型不同，CBOW模型不考虑上下文中单词的位置或者顺序，因此模型的输入实际上是一个“词袋”而非序列，这也是模型取名为“Continuous Bag-of-Words”的原因。但是，这并不意味着位置信息毫无用处。相关研究[7]表明，融入相对位置信息之后所得到的词向量在语法相关的自然语言处理任务（如词性标注、依存句法分析）上表现更好。这里只对其基本形式进行介绍。

​	CBOW模型可以表示成图5-4所示的前馈神经网络结构。与一般的前馈神经网络相比，CBOW模型的隐含层只是执行对词向量层取平均的操作，而没有线性变换以及非线性激活的过程。所以，也可以认为CBOW模型是没有隐含层的，这也是CBOW模型具有高训练效率的主要原因。

![](5-2-1.jpg)

​									图5-4 CBOW模型示意图

（1）输入层。以大小为5的上下文窗口为例，在目标词wt左右各取2个词作为模型的输入。输入层由4个维度为词表长度的独热表示向量构成。

（2）词向量层。输入层中每个词的独热表示向量经由矩阵映射至词向量空间：

![](5-2-2.jpg)

wi对应的词向量即为矩阵E中相应位置的列向量，E则为由所有词向量构成的矩阵或查找表。令Ct={wt−k，···， wt−1， wt+1，···， wt+k}表示wt的上下文单词集合，对Ct中所有词向量取平均，就得到了wt的上下文表示：

![](5-2-3.jpg)

（3）输出层。输出层根据上下文表示对目标词进行预测（分类），与前馈神经网络语言模型基本一致，唯一的不同在于丢弃了线性变换的偏置项。令 E′ ∈为隐含层到输出层的权值矩阵，记为E′中与wi对应的行向量，那么输出wt的概率可由下式计算：

![](5-2-4.jpg)

​	在CBOW模型的参数中，矩阵E和E′均可作为词向量矩阵，它们分别描述了词表中的词在作为条件上下文或目标词时的不同性质。在实际中，<u>通常只用E就能够满足应用需求</u>，但是在某些任务中，对两者进行组合得到的向量可能会取得更好的表现。

##### 5.2.1.2 Skip-gram模型

绝大多数词向量学习模型本质上都是在建立词与其上下文之间的联系。CBOW模型使用上下文窗口中词的集合作为条件输入预测目标词，即P （wt|Ct），其中Ct={wt−k，···， wt−1， wt+1，···， wt+k}。而Skip-gram模型在此基础之上作了进一进一步的简化，**使用Ct中的每个词作为独立的上下文对目标词进行预测**。因此，<u>*Skip-gram模型建立的是词与词之间的共现关系*</u>，即 P （wt|wt+j），其中 j ∈ {±1，···，±k}。原文献[6]对于Skip-gram模型的描述是根据当前词wt预测其上下文中的词wt+j，即P （wt+j|wt）。这两种形式是等价的，本章采用后一种形式对Skip-gram模型进行解释与分析。

![](5-2-5.jpg)

​									图5-5 Skip-gram模型示意图
​	仍然以k=2为例，Skip-gram模型可以表示为图5-5的结构，其中输入层是当前时刻wt的独热编码，通过矩阵E投射至隐含层。此时，隐含层向量即为wt的词向量。根据，输出层利用线性变换矩阵E′对上下文窗口内的词进行独立的预测：

![](5-2-6.jpg)

式中，c∈{wt−2， wt−1， wt+1， wt+2}。
​	与CBOW模型类似，Skip-gram模型中的权值矩阵E与E′均可作为词向量矩阵使用。

##### 5.2.1.3 参数估计

​	与神经网络语言模型类似，可以通过优化分类损失对 CBOW 模型和 Skip-gram模型进行训练，需要估计的参数为θ={E， E′}。例如，给定一段长为T 的词序列w1w2··· wT，CBOW模型的负对数似然损失函数为：

![](5-2-7.jpg)

式中，Ct={wt−k，···， wt−1， wt+1，···， wt+k}。
Skip-gram模型的负对数似然损失函数为：

![](5-2-8.jpg)

#### 5.2.2 负采样

​	目前介绍的词向量预训练模型可以归纳为对目标词的条件预测任务，如**`根据上下文预测当前词（CBOW模型）或者根据当前词预测上下文（Skip-gram模型）`**。当词表规模较大且计算资源有限时，这类模型的训练过程会受到输出层概率归一化（Normalization）计算效率的影响。负采样方法则提供了一种新的任务视角：给定当前词与其上下文，最大化两者共现的概率。这样一来，问题就被简化为对于（w， c）的二元分类问题（共现或者非共现），从而规避了大词表上的归一化计算。令P(D=1|w,c）表示c与w共现的概率：

![](5-2-9.jpg)

那么，两者不共现的概率则为：

![](5-2-10.jpg)

​	负采样算法适用于不同的（w，c）定义形式。例如，在Skip-gram模型中，w=wt， c= wt+j。若使用负采样方法估计，（wt， wt+j）则为满足共现条件的一对正样本，对应的类别D=1。与此同时，对c进行若干次负采样，得到K个不出现在wt上下文窗口内的词语，记为。对于，其类别D=0。
将式（5-14）中的对数似然log P （wt+j|wt）替换为如下形式：

![](5-2-11.jpg)

​	就得到了基于负采样方法的Skip-gram模型损失函数。其中，根据分布 Pn（w） 采样得到，即。假设P1（w） 表示从训练语料中统计得到的 Unigram 分布，目前被证明具有较好实际效果的一种负采样分布则为Pn（w）∝P1（w）3/4。
​	在CBOW模型中，通过对wt进行负采样，同样能够获得对应于正样本（Ct， wt）的负样本集合，进而采用同样的方法构建损失函数并进行参数估计。

#### 5.2.3 模型实现

​	本节将给出CBOW模型与Skip-gram模型的PyTorch实现。所有实现仍然沿用“数据+模型+训练算法”的框架。其中，CBOW与Skip-gram模型（非负采样）的训练算法与前面介绍的神经网络语言模型基本一致，这里不再赘述，只给出其数据类与模型类的实现方法。

##### 5.2.3.1 CBOW模型

（1）数据。首先定义 CBOW 模型的数据构建与存取模块 CbowDataset。CBOW模型的输入为一定上下文窗口内的词（集合），输出为当前词。

![](5-2-12.png)

![](5-2-13.jpg)

（2）模型。CBOW模型结构与前馈神经网络较为接近，区别在于隐含层完全线性化，只需要对输入层向量取平均。CbowModel类的实现如下。

![](5-2-14.png)

##### 5.2.3.2 Skip-gram模型

（1）数据。Skip-gram模型的数据输入输出与CBOW模型接近，主要区别在于输入输出都是单个词，即在一定上下文窗口大小内共现的词对。

![](5-2-15.png)

![](5-2-16.jpg)

（2）模型。Skip-gram模型的实现代码如下。

![](5-2-17.png)

##### 5.2.3.3 基于负采样的Skip-gram模型--需要进一步理解

（1）数据。在基于负采样的Skip-gram模型中，对于每个训练（正）样本，需要根据某个负采样概率分布生成相应的负样本，同时需要保证负样本不包含当前上下文窗口内的词。一种实现方式是，在构建训练数据的过程中就完成负样本的生成，这样在训练时直接读取负样本即可。这样做的优点是训练过程无须再进行负采样，因而效率较高；缺点是每次迭代使用的是同样的负样本，缺乏多样性。这里采用在训练过程中实时进行负采样的实现方式，通过数据类SGNSDataset的collate_fn函数完成负采样。

![](5-2-18.png)

![](5-2-19.png)

（2）模型。在模型类中维护两个词向量层w_embeddings和c_embeddings，分别用于词与上下文的向量表示。

![](5-2-20.jpg)

![](5-2-21.jpg)

（3）训练。首先，编写函数从训练语料中统计Unigram出现次数并计算概率分布。

![](5-2-22.png)

接下来是具体的训练过程，这里根据式（5-17）来计算总体损失函数，与前文神经网络语言模型直接使用负对数似然损失有所区别。

![](5-2-23.png)

![](5-2-24.png)

![](5-2-25.jpg)

### 5.3 GloVe词向量

#### 5.3.1 概述

​	无论是基于神经网络语言模型还是Word2vec的词向量预训练方法，本质上都是利用文本中词与词在局部上下文中的共现信息作为自监督学习信号。除此之外，另一类常用于估计词向量的方法是基于矩阵分解的方法，例如潜在语义分析（2.1节）等。这类方法首先对语料进行统计分析，并获得含有全局统计信息的“`词--上下文`”共现矩阵，然后利用**奇异值分解**（**`Singular Value Decomposition，SVD`**）对该矩阵进行降维，进而得到词的低维表示。然而，传统的矩阵分解方法得到的词向量不具备良好的几何性质，因此，文献[8]结合词向量以及矩阵分解的思想，提出了**`GloVe`**（**`Global Vectors for Word Representation`**）模型。

#### 5.3.2 预训练任务

​	GloVe模型的基本思想是**利用词向量对“词--上下文”共现矩阵进行预测（或者回归），从而实现隐式的矩阵分解**。首先，构建共现矩阵M，其中Mw，c表示词w与上下文c在受限窗口大小内的共现次数。GloVe模型在构建M 的过程中进一步考虑了w与c的距离，认为距离较远的（w， c）对于全局共现次数的贡献较小，因此采用以下基于共现距离进行加权的计算方式：

![](5-3-1.jpg)

式中，di（w， c）表示在第i次共现发生时，w与c之间的距离。
在获得矩阵M之后，利用词与上下文向量表示对M中的元素（取对数）进行回归计算。具体形式为：

![](5-3-2.jpg)

式中，vw、分别表示w与c的向量表示；bw与分别表示相应的偏置项。对以上回归问题进行求解，即可获得词与上下文的向量表示。

#### 5.3.3 参数估计

令θ={E， E′， b， b′}表示GloVe模型中所有可学习的参数，表示训练语料中所有共现的（w， c）样本集合。GloVe模型通过优化以下加权回归损失函数进行学习：

![](5-3-3.jpg)

​	式中，f （Mw，c）表示每一个（w， c）样本的权重。样本的权重与其共现次数相关。首先，共现次数很少的样本通常被认为含有较大的噪声，所蕴含的有用信息相对于频繁共现的样本也更少，因此希望给予较低的权重；其次，对于高频共现的样本，也需要避免给予过高的权重。因此，GloVe采用了以下的分段函数进行加权：

![](5-3-4.jpg)

​	当Mw，c不超过某个阈值（mmax）时，f （Mw，c）的值随Mw，c递增且小于或等于1，其增长速率由α控制；而当Mw，c＞mmax时，f （Mw，c）恒为1。

#### 5.3.4 模型实现

（1）数据。构建数据处理模块，该模块需要完成共现矩阵的构建与存取，具体实现如下。

![](5-3-5.png)

![](5-3-6.png)

（2）模型。GloVe模型与基于负采样的Skip-gram模型类似，唯一的区别在于增加了两个偏置向量，具体代码如下。

![](5-3-7.png)

（3）训练。在训练过程中，根据式（5-20）计算回归损失函数。具体代码如下。

![](5-3-8.png)

![](5-3-9.jpg)

### 5.4 评价与应用

​	对于不同的学习方法得到的词向量，通常可以根据其对词义相关性或者类比推理性的表达能力进行评价，这种方式属于**内部任务评价方法**（**`Intrinsic Evalua-tion`**）。在实际任务中，则需要根据下游任务的性能指标判断，也称为**外部任务评价方法**（**`Extrinsic Evaluation`**）。这里首先介绍两种常用的内部任务评价方法，然后以情感分类任务为例，介绍如何将预训练词向量应用于下游任务。

#### 5.4.1 词义相关性

​	对词义相关性的度量是词向量的重要性质之一。可以**根据词向量对词义相关性的表达能力衡量词向量的好坏。**
​	利用词向量低维、稠密、连续的特性，可以方便地度量任意两个词之间的相关性。例如，给定词wa与wb，它们在词向量空间内的**余弦相似度**就可以作为其词义相关性的度量：

![](5-4-1.jpg)

​	基于该相关性度量，定义以下函数实现K近邻（K-Nearest Neighbors，KNN）查询。

![](5-4-2.jpg)

利用该函数，可实现在词向量空间内进行近义词检索。

![](5-4-3.jpg)

![](5-4-4.jpg)

这里使用斯坦福大学发布的GloVe预训练词向量，该词向量是在大规模文本数据上使用GloVe算法训练得到，也是目前被广泛使用的预训练词向量之一。下载好词向量之后，使用 load_pretrained 函数进行加载，并返回词表与词向量对象。

![](5-4-5.png)

![](5-4-6.jpg)

在GloVe词向量空间内以“august”“good”为查询词进行近义词检索，可以得到以下结果。

![](5-4-7.png)

​	可见，词向量准确地反映了词义的相关性。
​	与此同时，还可以利用含有词义相关性的人工标注作为黄金标准，对词向量进行定量的评价。以目前常用的评价数据集——WordSim353为例：该数据集包含353个英文词对，每个词对由16位标注者给出[0，10]区间内的一个数值，最后取平均值作为该词对的词义相似度，如表5-1所示。由词向量计算得到的相似度值与人工标注值之间的相关系数（如Spearman或者Pearson相关系数）即可作为词向量评价的标准。

![](5-4-8.jpg)

​							表5-1 WordSim353数据集中的词义相似度标注示例

#### 5.4.2 类比性

​	词的**`类比性（Word analogy）`**是对于词向量的另一种常用的内部任务评价方法。对词向量在向量空间内的分布进行分析可以发现，对于语法或者语义关系相同的两个词对（wa， wb）与（wc， wd），它们的词向量在一定程度上满足：≈的几何性质。例如，在图5-6的示例中有以下类比关系：

![](5-4-9.jpg)

![](5-4-10.jpg)

​						图5-6 词向量空间内的语义和语法类比推理性质示例
​	这两个例子分别从词义和词法两个角度展示了词向量的类比性。根据这一性质，可以进行词与词之间的关系推理，从而回答诸如“wa之于wb，相当于wc之于？”的问题。对于下画线处的词，可以利用下式在词向量空间内进行搜索得到：

![](5-4-11.jpg)

​	利用前文的knn函数，可以方便地实现这一功能。具体代码如下：

![](5-4-12.jpg)

一般来说，词向量在以上评价方法中的表现与训练数据的来源及规模、词向量的维度等因素密切相关。在实际应用中，需要根据词向量在具体任务中的表现来选择。

#### 5.4.3 应用

​	预训练词向量可以作为词的特征表示直接用于下游任务，也可以作为模型参数在下游任务的训练过程中进行**`精调（Fine-tuning）`**。在通常情况下，两种方式都能够有效地提升模型的泛化能力。
​	第4章介绍了如何构建不同类型的神经网络模型，如多层感知器、循环神经网络等，来完成情感分析以及词性标注等自然语言处理任务。这些模型均使用了随机初始化的词向量层实现由离散词表示到连续向量表示的转换。为了利用已预训练好的词向量，只需要对词向量层的初始化过程进行简单的修改。以基于多层感知器模型的情感分类模型为例，具体代码如下。

![](5-4-13.png)

![](5-4-14.jpg)

​	由于下游任务训练数据的词表与预训练词向量的词表通常有所不同，因此，这里只初始化在预训练词表中存在的词，对于其他词则仍然保留其随机初始化向量，并在后续训练过程中精调。此外，读者也可以尝试其他的初始化方式。例如，可以根据预训练词向量确定词表，而对于其他词统一使用“＜unk＞”标记代替。在目标任务的训练过程中，有的情况下“冻结”词向量参数会取得更好的效果（可以通过设置requires_gradient=False来实现）。此时词向量被作为特征使用。
​	对于其他模型（如LSTM、Transformer等）的修改与之类似，请读者自行实现。
​	为了观察使用预训练词向量进行初始化带来的变化，在此沿用第4章采用的NLTK sentence_polarity数据进行实验，这里使用正负各1，000个样本。图5-7展示了其与使用随机初始化词向量层的模型在训练过程中损失函数的变化曲线。通过两者的对比可以看出，预训练词向量能够显著加快模型训练时的收敛速度。在10轮迭代之后，模型在测试集上的准确率为70%，相比于使用随机初始化词向量层的模型（67%），也取得了较为显著的提升。

![](5-4-15.jpg)

​							图5-7 两种模型训练过程中模型损失的变化曲线

## 第6章 动态词向量预训练模型

### 6.1 词向量-从静态到动态

​	如前文所述，词向量的学习主要利用了<u>语料库中词与词之间的共现信息</u>，其背后的`核心思想`是<u>分布式语义假设</u>。在目前介绍的静态词向量学习算法中，无论是<u>基于局部上下文预测的 Word2vec 算法</u>，还是<u>基于显式全局共现信息的 GloVe回归算法</u>，其本质都是<u>将一个词在整个语料库中的共现上下文信息聚合至该词的向量表示中</u>。因此，在一个给定的语料库上训练得到的词向量可以认为是“静态”的，即：对于任意一个词，其向量表示是恒定的，不随其上下文的变化而变化。
​	然而，在自然语言中，同一个词在不同的上下文或语境下可能呈现出多种不同的词义、语法性质或者属性。以“下场”一词为例，其在句子“他 亲自 下场 参加 比赛”和“竟 落得 这样 的 下场”中的词义截然不同，而且具有不同的词性（前者为动词，后者为名词）。一词多义是自然语言中普遍存在的语言现象，也是自然语言在发展变化过程中的自然结果。在静态词向量表示中，由于词的所有上下文信息都被压缩、聚合至单个向量表示内，因此难以刻画一个词在不同上下文或不同语境下的不同词义信息。
​	为了解决这一问题，研究人员提出了**`上下文相关的词向量（Contextualized Word Embedding）`**。顾名思义，在这种表示方法中，**`一个词的向量将由其当前所在的上下文计算获得，因此是随上下文而动态变化的`**。在本书中，也将其称为**`动态词向量（Dynamic Word Embedding）`**。在动态词向量表示下，前面例子中的“下场”在两句话中将分别得到两个不同的词向量表示。需要注意的是，动态词向量仍然严格满足分布式语义假设。
​	在一个文本序列中，每个词的动态词向量实际上是对该词的上下文进行语义组合后的结果。而对于文本这种序列数据而言，循环神经网络恰好提供了一种有效的语义组合方式。本书的第4章与第5章分别介绍了循环神经网络，以及在序列数据建模中的应用。在这些应用中，<u>既有利用循环神经网络最后时刻的隐含层表示作为整个文本片段（句子）的向量表示，以进行文本分类</u>；<u>也有利用每一时刻的隐含层表示进行序列标注（如词性标注）</u>。这意味着，**`循环神经网络模型中每一时刻（位置）的隐含层表示恰好可以作为该时刻词在当前上下文条件下的向量表示，即动态词向量`**。同时，循环神经网络可以通过语言模型任务进行自监督学习，而无须任何额外的数据标注。基于该思想，Matthew Peters等人在文献[9]中提出语言模型增强的序列标注模型TagLM。该模型利用预训练循环神经网络语言模型的隐含层表示作为额外的词特征，显著地提升了序列标注任务的性能。随后，他们进一步完善了这项研究，并提出深度上下文相关词向量的思想以及**`预训练模型ELMo（Embeddings from Language Models）`**[10]。在包括自动问答、文本蕴含和信息抽取等多项自然语言处理任务上的实验表明，ELMo能够直接有效地为当时最好的模型带来显著的提升。同时，ELMo模型还被推广至多语言场景，在CoNLL-2018国际多语言通用依存句法分析的评测任务中取得了优异的表现[11]。
​	在特定的条件下，也可以利用更丰富的监督信号训练循环神经网络。例如，当存在一定规模的双语平行语料时，可以利用基于序列到序列的机器翻译方法训练循环神经网络。在训练完成后，便可以利用翻译模型的编码器对源语言进行编码以获取动态词向量。文献[12]提出的CoVe模型采用了这种预训练方法。但是，双语平行语料的获取难度相比单语数据更高，且覆盖的领域也相对有限，因此通用性有所欠缺。
本章将主要介绍基于语言模型的动态词向量预训练方法，以及在自然语言处理任务中的典型应用。

### 6.2 基于语言模型的动态词向量预训练

#### 6.2.1 双向语言模型

​	对于给定的一段输入文本w1w2··· wn，双向语言模型从前向（从左到右）和后向（从右到左）两个方向同时建立语言模型。这样做的好处在于，对于文本中任一时刻的词wt，可以同时获得其分别基于左侧上下文信息和右侧上下文信息的表示。
​	具体地，模型首先对每个词单独编码。这一过程是上下文无关的，主要利用了词内部的字符序列信息。基于编码后的词表示序列，模型使用两个不同方向的多层长短时记忆网络（LSTM）分别计算每一时刻词的前向、后向隐含层表示，也就是上下文相关的词向量表示。利用该表示，模型预测每一时刻的目标词。对于前向语言模型，t时刻的目标词是wt+1；对于后向语言模型，目标词是wt−1。（1）输入表示层。ELMo模型采用基于字符组合的神经网络表示输入文本中的每个词，目的是减小未登录词（Out-Of-Vocabulary，OOV）对模型的影响。图6-1展示了输入表示层的基本结构。
​	首先，字符向量层将输入层中的每个字符（含额外添加的起止符）转换为向量表示。假设wt由字符序列c1c2··· cl构成，对于其中的每个字符ci，可以表示为：



## 第七章 预训练语言模型

### 7.1 概述

第6章介绍的动态词向量方法CoVe和ELMo将词表示从静态转变到动态，同时也在多个自然语言处理任务中显著地提升了性能。随后，以GPT和BERT为代表的基于大规模文本训练出的预训练语言模型（Pre-trained Language Model，PLM）已成为目前主流的文本表示模型。本章首先介绍预训练语言模型的特点及主要组成部分，让读者对预训练语言模型有一个基本的认识。然后介绍以GPT为代表的基于自回归的预训练语言模型。接着介绍基于自编码的预训练语言模型，并以经典的BERT为例，详细介绍其建模方法。最后，以多个典型的自然语言处理任务为例，结合相关代码介绍预训练语言模型在下游任务中的应用方法。

预训练模型并不是自然语言处理领域的“首创”技术。在计算机视觉（Com-puter Vision，CV）领域，以ImageNet[14]为代表的大规模图像数据为图像识别、图像分割等任务提供了良好的数据基础。因此，在计算机视觉领域，通常会使用Ima-geNet进行一次预训练，让模型从海量图像中充分学习如何从图像中提取特征。然后，会根据具体的目标任务，使用相应的领域数据精调，使模型进一步“靠近”目标任务的应用场景，起到领域适配和任务适配的作用。这好比人们在小学、初中和高中阶段会学习数学、语文、物理、化学和地理等基础知识，夯实基本功并构建基本的知识体系（预训练阶段）。而当人们步入大学后，将根据选择的专业（目标任务）学习某一领域更深层次的知识（精调阶段）。从以上介绍中可以看出，“预训练+精调”模式在自然语言处理领域的兴起并非偶然现象。
由于自然语言处理的核心在于如何更好地建模语言，所以在自然语言处理领域中，预训练模型通常指代的是预训练语言模型。广义上的预训练语言模型可以泛指提前经过大规模数据训练的语言模型，包括早期的以Word2vec、GloVe为代表的静态词向量模型，以及基于上下文建模的CoVe、ELMo等动态词向量模型。在2018年，以GPT和BERT为代表的基于深层Transformer的表示模型出现后，预训练语言模型这个词才真正被大家广泛熟知。因此，目前在自然语言处理领域中提到的预训练语言模型大多指此类模型。预训练语言模型的出现使得自然语言处理进入新的时代，也被认为是近些年来自然语言处理领域中的里程碑事件。
相比传统的文本表示模型，预训练语言模型具有“三大”特点——大数据、大模型和大算力。接下来介绍这“三大”特点代表的具体含义。

#### 7.1.1 大数据

“工欲善其事，必先利其器。”要想学习更加丰富的文本语义表示，就需要获取文本在不同上下文中出现的情况，因此大规模的文本数据是必不可少的。获取足够多的大规模文本数据是训练一个好的预训练语言模型的开始。因此，预训练数据需要讲究“保质”和“保量”。
•“保质”是希望预训练语料的质量要尽可能高，避免混入过多的低质量语料。这与训练普通的自然语言处理模型的标准基本是一致的；
•“保量”是希望预训练语料的规模要尽可能大，从而获取更丰富的上下文信息。

在实际情况中，预训练数据往往来源不同。精细化地预处理所有不同来源的数据是非常困难的。因此，在预训练数据的准备过程中，通常不会进行非常精细化地处理，仅会预处理语料的共性问题。同时，通过增大语料规模进一步稀释低质量语料的比重，从而降低质量较差的语料对预训练过程带来的负面影响。当然，预训练语料的质量越高，训练出来的预训练语言模型的质量也相对更好，这需要在数据处理投入和数据质量之间做出权衡。

#### 7.1.2 大模型

在有了大数据后，就需要有一个足以容纳这些数据的模型。数据规模和模型规模在一定程度上是正相关的。当在小数据上训练模型时，通常模型的规模不会太大，以避免出现过拟合现象。而当在大数据上训练模型时，如果不增大模型规模，可能会造成新的知识无法存放的情况，从而无法完全涵盖大数据中丰富的语义信息。因此，需要一个容量足够大的模型来学习和存放大数据中的各种特征。在机器学习中，“容量大”通常指的是模型的“参数量大”。那么，如何设计这样一个参数量较大的模型呢？这里主要考虑以下两个方面。
• 模型需要具有较高的并行程度，以弥补大模型带来的训练速度下降的问题；
• 模型能够捕获并构建上下文信息，以充分挖掘大数据文本中丰富的语义信息。
综合以上两点条件，基于Transformer的神经网络模型成为目前构建预训练语言模型的最佳选择。首先，Transformer模型具有较高的并行程度。Transformer核心部分的多头自注意力机制（Multi-head Self-attention）[15]不依赖于顺序建模，因此可以快速地并行处理。与此相反，传统的神经网络语言模型通常基于循环神经网络（RNN），而RNN需要按照序列顺序处理，并行化程度较低。其次，Transformer中的多头自注意力机制能够有效地捕获不同词之间的关联程度，并且能够通过多头机制从不同维度刻画这种关联程度，使得模型能够得到更加精准的计算结果。因此，主流的预训练语言模型无一例外都使用了Transformer作为模型的主体结构。

#### 7.1.3 大算力

即使拥有了大数据和大模型，但如果没有与之相匹配的大算力，预训练语言模型也很难得以实现。为了训练预训练语言模型，除了大家熟知的深度学习计算设备——图形处理单元（Graphics Processing Unit，GPU），还有后起之秀——张量处理单元（Tensor Processing Unit，TPU）。下面就这两种常见的深度学习计算设备进行简单的介绍。1.图形处理单元图形处理单元（GPU，俗称显卡）是大家最熟悉的计算设备之一。早期，GPU主要用来处理计算机图形，是连接计算机主机和显示终端（如显示器）的纽带。而随着GPU核心的不断升级，在其计算能力和计算速度得到大幅提升后，不仅可以作为常规的图形处理设备，同时也可以成为深度学习领域的计算设备。
那么，为什么不使用中央处理器（Central Processing Unit，CPU）来运行深度学习任务呢？因为CPU和GPU擅长的任务类型是不同的。CPU擅长处理串行运算以及逻辑控制和跳转，而GPU更擅长大规模并行运算。由于深度学习中经常涉及大量的矩阵或张量之间的计算，并且这些计算是可以并行完成的，所以特别适合用GPU处理。
目前，在深度学习领域应用范围最广的GPU品牌是英伟达（NVIDIA）。英伟达生产的GPU依靠与之匹配的统一计算设备架构（Compute Unified Device Archi-tecture，CUDA）能够更好地处理复杂的计算问题，同时深度优化多种深度学习基本运算指令。大家熟知的PyTorch、TensorFlow等主流的深度学习框架均提供了基于CUDA的GPU运算支持，并提供了更高层、更抽象的调用方式，使得用户可以更方便地编写深度学习程序。
目前广受欢迎的深度学习设备是英伟达Volta系列硬件，其中最为人熟知的型号是V100，其在深度学习框架下的浮点运算性能达到了125 TFLOPS（以NVLink版为例）。V100的人工智能推理吞吐量比CPU高出20倍以上，并且在高性能计算（High Performance Computing，HPC）方面相比CPU高出100倍以上。2.张量处理单元张量处理单元（TPU）[16]是谷歌公司近年定制开发的专用集成电路（Appli-cation Specific Integrated Circuit，ASIC），专门用于加快机器学习任务的训练，但在早期并没有像GPU那样被广为熟知。研究人员能够使用TensorFlow在TPU加速器硬件上快速地完成机器学习任务的训练。TPU提高了机器学习应用中大量使用的线性代数计算的性能。当训练大型复杂的神经网络模型时，TPU可以大幅度缩短达到既定准确率所需的时间，提高模型的收敛速度。例如，以前在其他硬件平台上需要花费数周时间进行训练的深度学习模型，在TPU上只需数小时即可完成训练。同时，借助谷歌公司开发的TensorFlow深度学习框架以及对TPU硬件的针对性优化，研究人员可以借助TensorFlow提供的API，方便地将模型迁移到TPU硬件上运行。目前，TPU主要支持TensorFlow深度学习框架，并逐步完善对PyTorch深度学习框架的支持，基本满足了大多数相关从业人员的需求。
图7-1给出了两种常用TPU的硬件架构图，包括TPU v2和TPU v3。每个TPU版本定义了TPU设备的特定硬件特征，其中包括每个TPU核心的架构、高带宽内存（HBM）的数量、每个TPU设备上核心之间的互连和可用于设备间通信的网络接口。TPU v2和TPU v3之间的属性对比如表7-1所示。
	与分布式GPU类似，谷歌数据中心中的TPU Pod是通过专用高速网络相互连接的多TPU设备。TPU节点中的主机在所有TPU设备上分配机器学习工作负载。在TPU Pod中，TPU芯片在设备上互连，同时通过专用高速网络互连，因此芯片之间的通信无需主机CPU或主机网络资源。由TPU v2构成的TPU v2 Pod可最高拥有512个TPU核心和4 TB的总内存。而TPU v3 Pod可进一步将核心数提升至2048个，并且提供高达32 TB的总内存。由于可以提供超大算力和内存，TPU Pod也是目前训练超大规模预训练语言模型的首选设备之一。

### 7.2 GPT

OpenAI 公司在2018年提出了一种生成式预训练（Generative Pre-Training，GPT）模型[17]用来提升自然语言理解任务的效果，正式将自然语言处理带入“预训练”时代。“预训练”时代意味着利用更大规模的文本数据以及更深层的神经网络模型学习更丰富的文本语义表示。同时，GPT的出现打破了自然语言处理各个任务之间的壁垒，使得搭建一个面向特定任务的自然语言处理模型不再需要了解非常多的任务背景，只需要根据任务的输入输出形式应用这些预训练语言模型，就能够达到一个不错的效果。因此，GPT提出了“生成式预训练+判别式任务精调”的自然语言处理新范式，使得自然语言处理模型的搭建变得不再复杂。
• 生成式预训练：在大规模文本数据上训练一个高容量的语言模型，从而学习更加丰富的上下文信息；
• 判别式任务精调：将预训练好的模型适配到下游任务中，并使用有标注数据学习判别式任务。
接下来将从两个部分介绍GPT模型。首先介绍GPT模型的基本结构及其预训练方法，然后介绍GPT模型在不同下游任务中的应用。

#### 7.2.1 无监督预训练

GPT的整体结构是一个基于Transformer的单向语言模型，即从左至右对输入文本建模，如图7-2所示。

![](41512_200_1.jpg)

​							图7-2 GPT的整体模型结果

GPT利用常规语言建模的方法优化给定文本序列x=x1··· xn的最大似然估计LPT。

![](41512_201_1.jpg)

式中，k表示语言模型的窗口大小，即基于k个历史词xi−k··· xi−1预测当前时刻的词xi；θ表示神经网络模型的参数，可使用随机梯度下降法优化该似然函数。
具体地，GPT使用了多层Transformer作为模型的基本结构。由于在4.4.3节中已经介绍了Transformer的内部结构，因此这里不再赘述。对于长度为k的窗口词序列x′=x−k···x−1，通过以下方式计算建模概率P。

​	![](41512_201_2.jpg)

式中，![](41512_201_3.jpg)表示 x′ 的独热向量表示；![](41512_201_4.jpg)表示词向量矩阵；![](41512_201_5.jpg)表示位置向量矩阵（此处只截取窗口 x′ 对应的位置向量）；L表示Transformer的总层数。

#### 7.2.2 有监督下游任务精调

​	在预训练阶段，GPT利用大规模数据训练出基于深层Transformer的语言模型，已经掌握了文本的通用语义表示。精调（Fine-tuning）的目的是在通用语义表示的基础上，根据下游任务（Downstream task）的特性进行领域适配，使之与下游任务的形式更加契合，以获得更好的下游任务应用效果。接下来，将介绍如何将预训练好的GPT应用在实际的下游任务中。
​	下游任务精调通常是由有标注数据进行训练和优化的。假设下游任务的标注数据为C，其中每个样例的输入是x=x1···xn构成的长度为n的文本序列，与之对应的标签为y。首先将文本序列输入预训练的GPT中，获取最后一层的最后一个词对应的隐含层输出![](41512_201_6.jpg)，如式（7-3）所示。紧接着，将该隐含层输出通过一层全连接层变换，预测最终的标签。

![](41512_201_7.jpg)

式中，![](41512_201_8.jpg)表示全连接层权重，k表示标签个数。

最终，通过优化以下损失函数精调下游任务。

![](41512_201_9.jpg)

另外，为了进一步提升精调后模型的通用性以及收敛速度，可以在下游任务精调时加入一定权重的预训练任务损失。这样做是为了缓解在下游任务精调的过程中出现灾难性遗忘（Catastrophic Forgetting）问题。因为在下游任务精调过程中，GPT的训练目标是优化下游任务数据上的效果，更强调特殊性。因此，势必会对预训练阶段学习的通用知识产生部分的覆盖或擦除，丢失一定的通用性。通过结合下游任务精调损失和预训练任务损失，可以有效地缓解灾难性遗忘问题，在优化下游任务效果的同时保留一定的通用性。在实际应用中，可通过下式精调下游任务。

![](41512_202_1.jpg)

式中，![](41512_202_2.jpg)表示精调任务损失；![](41512_202_3.jpg)表示预训练任务损失；λ表示权重，通常λ的取值介于[0，1]。
特别地，当λ=0时，![](41512_202_3.jpg)一项无效，表示只使用精调任务损失![](41512_202_2.jpg)优化下游任务。而当λ=1时，![](41512_202_3.jpg)和![](41512_202_2.jpg)具有相同的权重。在实际应用中，通常设置λ=0.5，因为在精调下游任务的过程中，主要目的还是要优化有标注数据集的效果，即优化。而的引入主要是为了提升精调模型的通用性，其重要程度不及，因此设置 λ=0.5是一个较为合理的值（不同任务之间可能有一定的区别）。

#### 7.2.3 适配不同的下游任务

7.2.2节描述了GPT在下游任务精调的通用做法。但不同任务之间的输入形式各不相







### 7.3 BERT

​	BERT（Bidirectional Encoder Representation from Transformers）[18]是由Devlin等人在2018年提出的基于深层Transformer的预训练语言模型。BERT不仅充分利用了大规模无标注文本来挖掘其中丰富的语义信息，同时还进一步加深了自然语言处理模型的深度。
​	这一节将着重介绍BERT的建模方法，其中包括两个基本的预训练任务以及两个进阶预训练任务。最后，介绍如何利用BERT在四类典型的自然语言处理任务上快速搭建相应的模型，并结合代码实现进行实战。

#### 7.3.1 整体结构

​	首先，从整体框架的角度对BERT进行介绍，了解其基本的组成部分，然后针对每个部分详细介绍。BERT的基本模型结构由多层Transformer构成，包含两个预训练任务：掩码语言模型（Masked Language Model，MLM）和下一个句子预测（Next Sentence Prediction，NSP），如图7-4所示。

![](41512_204_1.jpg)

​								图7-4 BERT的整体模型结构
可以看到，模型的输入由两段文本x（1）和x（2）拼接组成，然后通过BERT建模得到上下文语义表示，最终学习掩码语言模型和下一个句子预测。需要注意的是，掩码语言模型对输入形式并没有特别要求，可以是一段文本也可以是两段文本。而下一个句子预测要求模型的输入是两段文本。因此，BERT在预训练阶段的输入形式统一为两段文本拼接的形式。接下来介绍如何对两段文本建模，得到对应的输入表示。

#### 7.3.2 输入表示

​	BERT的输入表示（Input Representation）由词向量（Token Embeddings）、块向量（Segment Embeddings）和位置向量（Position Embeddings）之和组成，如图7-5所示。

![](41512_205_1.jpg)

​										图7-5 BERT的输入表示
为了计算方便，在BERT中，这三种向量维度均为**e**，因此可通过下式计算输入序列对应的输入表示**v**：

![](41512_205_2.jpg)

式中，**vt**表示词向量；**vs**表示块向量；**vp**表示位置向量；三种向量的大小均为**N× e**，**N**表示序列最大长度，**e**表示词向量维度。接下来介绍这三种向量的计算方法。

（1）**词向量**。与传统神经网络模型类似，BERT中的词向量同样通过词向量矩阵将输入文本转换成实值向量表示。具体地，假设输入序列x对应的独热向量表示为![](41512_205_3.jpg)，其对应的词向量表示vt为：

![](41512_205_4.jpg)

式中，![](41512_205_5.jpg)表示可训练的词向量矩阵；**|V|**表示词表大小；**e**表示词向量维度。

（2）**块向量**。块向量用来编码当前词属于哪一个块（Segment）。输入序列中每个词对应的块编码（Segment Encoding）为当前词所在块的序号（从0开始计数）。

- 当输入序列是单个块时（如单句文本分类），所有词的块编码均为0；

-  当输入序列是两个块时（如句对文本分类），第一个句子中每个词对应的块编码为0，第二个句子中每个词对应的块编码为1。

  需要注意的是，**[CLS]**位（输入序列中的第一个标记）和第一个块结尾处的**[SEP]**位（用于分隔不同块的标记）的块编码均为0。接下来，利用块向量矩阵Ws将块编码![](41512_206_1.jpg)转换为实值向量，得到块向量**vs**：

  ![](41512_206_2.jpg)

  式中，![](41512_206_3.jpg)表示可训练的块向量矩阵；![](41512_206_4.jpg)表示块数量；**e**表示块向量维度。

（3）**位置向量**。位置向量用来编码每个词的绝对位置。将输入序列中的每个词按照其下标顺序依次转换为位置独热编码![](41512_206_5.jpg)。下一步，利用位置向量矩阵**Wp**将位置独热编码转换为实值向量，得到位置向量**vp**：

![](41512_206_6.jpg)

式中，![](41512_206_7.jpg)表示可训练的位置向量矩阵；**N**表示最大位置长度；**e**表示位置向量维度。
为了描述方便，后续输入表示层的操作统一归纳为式（7-12）。

![](41512_206_8.jpg)

对于给定的原始输入序列**X**，经过如下处理得到BERT的输入表示**v**：

![](41512_206_9.jpg)

式中，![](41512_206_10.jpg)表示输入表示层的最终输出结果，即词向量、块向量和位置向量之和；**N** 表示最大序列长度；**e**表示输入表示维度。

#### 7.3.3 基本预训练任务

​	与GPT不同的是，BERT并没有采用传统的基于自回归的语言建模方法，而是引入了基于自编码（Auto-Encoding）的预训练任务进行训练。BERT的基本预训练任务由掩码语言模型和下一个句子预测构成。下面详细介绍两个基本预训练任务。

##### 1.掩码语言模型

​	传统基于条件概率建模的语言模型只能从左至右（顺序）或者是从右至左（逆序）建模文本序列。如果同时进行顺序建模和逆序建模文本，则会导致信息泄露。顺序建模表示根据“历史”的词预测“未来”的词。与之相反，逆序建模是根据“未来”的词预测“历史”的词。如果对上述两者同时建模则会导致在顺序建模时“未来”的词已被逆序建模暴露，进而语言模型倾向于从逆序建模中直接输出相应的词，而非通过“历史”词推理预测，从而使得整个语言模型变得非常简单，无法学习深层次的语义信息。对于逆序建模，同样会遇到类似的问题。由于这种问题的存在，在第6章中提到的ELMo模型采用了独立的前向和后向两个语言模型建模文本。

​	为了真正实现文本的双向建模，即当前时刻的预测同时依赖于“历史”和“未来”，BERT采用了一种类似完形填空（Cloze）的做法，并称之为掩码语言模型（MLM）。MLM预训练任务直接将输入文本中的部分单词掩码（Mask），并通过深层Transformer模型还原为原单词，从而避免了双向语言模型带来的信息泄露问题，迫使模型使用被掩码词周围的上下文信息还原掩码位置的词。

​	在BERT中，采用了15%的掩码比例，即输入序列中15%的WordPieces子词被掩码。当掩码时，模型使用 [MASK]标记替换原单词以表示该位置已被掩码。然而，这样会造成预训练阶段和下游任务精调阶段之间的不一致性，因为人为引入的 [MASK]标记并不会在实际的下游任务中出现。为了缓解这个问题，当对输入序列掩码时，并非总是将其替换为 [MASK]标记，而会按概率选择以下三种操作中的一种：

- 以80%的概率替换为 [MASK]标记；

- 以10%的概率替换为词表中的任意一个随机词；

- 以10%的概率保持原词不变，即不替换。

  表7-2给出了三种掩码方式的示例。可以看到，当要预测 [MASK]标记对应的单词时，模型不仅需要理解当前空缺位置之前的词，同时还要理解空缺位置之后的词，从而达到了双向语言建模的目的。在了解MLM预训练任务的基本方法后，接下来介绍其建模方法。

  ![](41512_207_1.jpg)

  ​										表7-2 MLM任务训练样本示例

  （1）**输入层**。由于掩码语言模型并不要求输入一定是两段文本，为了描述方便，假设原始输入文本为**x1x2··· xn**，通过上述方法掩码后的输入文本为![](41512_207_2.jpg)，**xi**表示输入文本的第**i**个词，![](41512_207_3.jpg)表示经过掩码处理后的第**i**个词。对掩码后的输入文本进行如下处理，得到BERT的输入表示**v**：

  ![](41512_208_1.jpg)

  ​	式中，[CLS]表示文本序列开始的特殊标记；[SEP]表示文本序列之间的分隔标记。

  ​	需要注意的是，如果输入文本的长度n小于BERT的最大序列长度N，需要将补齐标记（Padding Token）[PAD]拼接在输入文本后，直至达到BERT的最大序列长度N。例如，在下面的例子中，假设BERT的最大序列长度N=10，而输入序列长度为7（两个特殊标记加上x1至x5），需要在输入序列后方添加3个[PAD]补齐标记。
  **[CLS] x1x2x3x4x5[SEP] [PAD] [PAD] [PAD]**
  ​	而如果输入序列X 的长度大于BERT的最大序列长度N，需要对输入序列X截断至BERT的最大序列长度N。例如，在下面的例子中，假设BERT的最大序列长度N=5，而输入序列长度为7（两个特殊标记加上x1至x5），需要对序列截断，使有效序列（输入序列中去除2个特殊标记）长度变为3。
  **[CLS] x1x2x3[SEP]**

  ​	为了描述方便，后续将忽略补齐标记 [PAD]的处理，并以N 表示最大序列长度。

  （2） **BERT 编码层**。在 BERT 编码层中，BERT 的输入表示 v 经过 L 层Transformer，借助自注意力机制充分学习文本中的每个词之间的语义关联。由于Transformer的编码方法已在4.4.3节中描述，此处不再赘述。

  ![](41512_208_2.jpg)

  式中，![](41512_208_3.jpg)表示第l层Transformer的隐含层输出，同时规定**h[0]=v**，以保持式（7-16）的完备性。为了描述方便，略去层与层之间的标记并简化为：

  ![](41512_208_4.jpg)

  式中，h表示最后一层Transformer的输出，即h[L]。通过上述方法最终得到文本的上下文语义表示![](41512_208_5.jpg)，其中d表示BERT的隐含层维度。

  （3）**输出层**。由于掩码语言模型仅对输入文本中的部分词进行了掩码操作，因此并不需要预测输入文本中的每个位置，而只需预测已经掩码的位置。假设集合![](41512_208_6.jpg)表示所有掩码位置的下标，k表示总掩码数量。如果输入文本长度为n，掩码比例为15%，则![](41512_209_1.jpg)。然后，以集合M中的元素为下标，从输入序列的上下文语义表示h中抽取出对应的表示![](41512_209_3.jpg)，并将这些表示进行拼接得到掩码表示。

  ​	在BERT中，由于输入表示维度e和隐含层维度d相同，可直接利用词向量矩阵![](41512_209_4.jpg)（式7-9）将掩码表示映射到词表空间。对于掩码表示中的第i个分量![](41512_209_5.jpg)，通过下式计算该掩码位置对应的词表上的概率分布Pi。

  ![](41512_209_6.jpg)

  式中，![](41512_209_7.jpg)表示全连接层的偏置。
  最后，在得到掩码位置对应的概率分布**Pi**后，与标签**yi**（即原单词xi的独热向量表示）计算交叉熵损失，学习模型参数。

  （4）**代码实现**。为了使读者加深对MLM预训练任务的理解，此处给出BERT原版的生成MLM训练数据的方法，并详细介绍其中的重点操作。

  ![](41512_209_8.jpg)

  ![](41512_210_1.jpg)

  ```python
  def create_masked_lm_predictions(tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng):
      cand_indexes = []
      for (i, token) in enumerate(tokens):
          if token == "CLS" or token == "[SEP]":
              continue
          cand_indexes.append([i])
  
          rng.shuffle(cand_indexes)
          output_tokens = list(tokens)
          num_to_predict = min(max_predictions_per_seq, max(1, int(round(len(tokens) * masked_lm_prob))))
          masked_lms = []
          covered_indexes = set()
          for index in cand_indexes:
              if len(masked_lms) >= num_to_predict:
                  break
              if index in covered_indexes:
                  continue
              covered_indexes.add(index)
              masked_token = None
  
              if rng.random() < 0.8:
                  masked_token = "[MASK]"
              else:
                  if rng.random() < 0.5:
                      masked_token = tokens[index]
                  else:
                      masked_token = vocab_words[rng.randint(0, len(vocab_words) - 1)]
              output_tokens[index] = masked_token
              masked_lms.append(MaskedInstance(index=index, label=tokens[index]))
          masked_lms = sorted(masked_lms, key=lambda x: x.index)
  
          masked_lm_positions = []
          masked_lm_labels = []
          for p in masked_lms:
              masked_lm_positions.append(p.index)
              masked_lm_labels.append(p.label)
          return (output_tokens, masked_lm_positions, masked_lm_labels)
  
  ```

##### 2.下一个句子预测

​	在MLM预训练任务中，模型已经能够根据上下文还原掩码部分的词，从而学习上下文敏感的文本表示。然而，对于阅读理解、文本蕴含等需要两段输入文本的任务来说，仅依靠MLM无法显式地学习两段输入文本之间的关联。例如，在阅读理解任务中，模型需要对篇章和问题建模，从而能够找到问题对应的答案；在文本蕴含任务中，模型需要分析输入的两段文本（前提和假设）的蕴含关系。
​	因此，除了MLM任务，BERT还引入了第二个预训练任务——下一个句子预测（NSP）任务，以构建两段文本之间的关系。NSP任务是一个二分类任务，需要判断句子B是否是句子A的下一个句子，其训练样本由以下方式产生。

- **正样本**：<u>来自自然文本中相邻的两个句子“句子A”和“句子B”，即构成“下一个句子”关系；</u>
- **负样本**：<u>将“句子B”替换为语料库中任意一个其他句子，即构成“非下一个句子”关系。</u>

​        NSP任务整体的正负样本比例控制在**1：1**。由于NSP任务的设计原则较为简单，通过上述方法能够自动生成大量的训练样本，所以也可以看作一个<u>无监督学习任务</u>。表7-3给出了NSP任务的样本示例。

![](41512_211_1.jpg)

​										表7-3 NSP任务的样本示例
​        NSP任务的建模方法与MLM任务类似，主要是在输出方面有所区别。下面针对NSP任务的建模方法进行说明。

（1）**输入层**。对于给定的经过掩码处理后的输入文本：

![](41512_211_2.jpg)

经过如下处理，得到BERT的输入表示**v**。

![](41512_211_3.jpg)

式中，**[CLS]**表示文本序列开始的特殊标记；**[SEP]**表示文本序列之间的分隔标记。

（2） **BERT编码层**。在BERT编码层中，输入表示v经过L层Transformer的编码，借助自注意力机制充分学习文本中每个词之间的语义关联，最终得到输入文本的上下文语义表示。

![](41512_211_4.jpg)

式中，![](41512_211_5.jpg)，其中N 表示最大序列长度，d表示BERT的隐含层维度。

（3）**输出层**。与MLM任务不同的是，NSP任务只需要判断输入文本x（2）是否是x（1）的下一个句子。因此，在NSP任务中，BERT使用了[CLS]位的隐含层表示进行分类预测。具体地，[CLS]位的隐含层表示由上下文语义表示h的首个分量h0构成，因为[CLS]是输入序列中的第一个元素。在得到[CLS]位的隐含层表示h0后，通过一个全连接层预测输入文本的分类概率![](41512_212_1.jpg)。

![](41512_212_2.jpg)

式中，![](41512_212_3.jpg)表示全连接层的权重；![](41512_212_4.jpg)表示全连接层的偏置。
最后，在得到分类概率P 后，与真实分类标签y计算交叉熵损失，学习模型参数。

